{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extractor(row):\n",
    "    \n",
    "    # defining path to train files\n",
    "    file_name = os.path.join(os.path.abspath('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/'),\n",
    "                            'Train', str(row.ID) + '.wav')\n",
    "    \n",
    "    if (row.ID%100 ==0):\n",
    "        print (\"Handling Row:\", row.ID)\n",
    "    \n",
    "    #enclosing all this in try-catch to handle exceptions and missing files\n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        \n",
    "        #calculating MFCC feature\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=39).T,axis=0) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculating RMS feature for 400 frames\n",
    "        from madmom.audio.signal import FramedSignal\n",
    "        from madmom.audio.signal import root_mean_square\n",
    "\n",
    "        fs = FramedSignal(file_name, num_channels=1,\n",
    "                          num_frames=400)\n",
    "        ##rounding rms values to the nearest integer \n",
    "        rms = np.around(root_mean_square(fs))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculating ZCR feature\n",
    "        from librosa.feature import zero_crossing_rate\n",
    "        from librosa.core import load \n",
    "\n",
    "        ##padding with zeros to make a constant length of feature across all audios\n",
    "        zcr_padded = np.zeros((210,))\n",
    "        zcr = zero_crossing_rate(X, frame_length=2048, hop_length=441)\n",
    "        zcr_padded[:zcr[0].shape[0]] = zcr[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculating PSD feature\n",
    "        from scipy import signal\n",
    "        from madmom.audio.signal import Signal\n",
    "\n",
    "        ##calculating PSD using Welch's method\n",
    "        s = Signal(file_name, num_channels=1)\n",
    "        f, P = signal.welch(s, 44100, nperseg=1024)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None, None\n",
    "\n",
    "    features = np.concatenate ((mfccs, rms, zcr_padded, P))\n",
    "    label = row.Class\n",
    "\n",
    "    return [features, label]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Row: 0\n",
      "Handling Row: 0\n",
      "Handling Row: 100\n",
      "Handling Row: 200\n",
      "Handling Row: 300\n",
      "Handling Row: 400\n",
      "Handling Row: 500\n",
      "Handling Row: 600\n",
      "Handling Row: 900\n",
      "Handling Row: 1000\n",
      "Handling Row: 1100\n",
      "Handling Row: 1200\n",
      "Handling Row: 1300\n",
      "Handling Row: 1400\n",
      "Handling Row: 1500\n",
      "Handling Row: 1800\n",
      "Handling Row: 2000\n",
      "Handling Row: 2200\n",
      "Handling Row: 2300\n",
      "Handling Row: 2600\n",
      "Handling Row: 2700\n",
      "Handling Row: 2800\n",
      "Handling Row: 2900\n",
      "Handling Row: 3200\n",
      "Handling Row: 3400\n",
      "Handling Row: 3700\n",
      "Handling Row: 3800\n",
      "Handling Row: 3900\n",
      "Handling Row: 4000\n",
      "Handling Row: 4100\n",
      "Handling Row: 4200\n",
      "Handling Row: 4400\n",
      "Handling Row: 4500\n",
      "Handling Row: 4800\n",
      "Handling Row: 4900\n",
      "Handling Row: 5100\n",
      "Handling Row: 5200\n",
      "Handling Row: 5400\n",
      "Handling Row: 5600\n",
      "Handling Row: 5800\n",
      "Handling Row: 5900\n",
      "Handling Row: 6100\n",
      "Handling Row: 6300\n",
      "Handling Row: 6500\n",
      "Handling Row: 6600\n",
      "Handling Row: 6800\n",
      "Handling Row: 6900\n",
      "Handling Row: 7000\n",
      "Handling Row: 7100\n",
      "Handling Row: 7200\n",
      "Handling Row: 7400\n",
      "Handling Row: 7500\n",
      "Handling Row: 7600\n",
      "Handling Row: 7700\n",
      "Handling Row: 7800\n",
      "Handling Row: 7900\n",
      "Handling Row: 8000\n",
      "Handling Row: 8100\n",
      "Handling Row: 8200\n",
      "Handling Row: 8300\n",
      "Handling Row: 8500\n"
     ]
    }
   ],
   "source": [
    "#applying feature_estractor on each row in train \n",
    "new_train = train.apply(feature_extractor, axis=1)\n",
    "new_train.columns = ['features', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1162,)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping rows for missing files\n",
    "new_train.dropna(inplace=True)\n",
    "new_train.features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preparing data for feeding into the model\n",
    "X = np.array(new_train.features.tolist())\n",
    "y = np.array(new_train.label.tolist())\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y = np_utils.to_categorical(lb.fit_transform(y))\n",
    "\n",
    "num_labels = y.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building the model architecture\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1024, input_shape=(1162,), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1024, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(512, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd= SGD(momentum=0.2, lr=0.03)\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "    return model\n",
    "          \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5435/5435 [==============================] - 4s 764us/step - loss: 3.2056 - acc: 0.1231\n",
      "Epoch 2/200\n",
      "5435/5435 [==============================] - 3s 613us/step - loss: 2.3533 - acc: 0.1603\n",
      "Epoch 3/200\n",
      "5435/5435 [==============================] - 3s 638us/step - loss: 2.2701 - acc: 0.1704\n",
      "Epoch 4/200\n",
      "5435/5435 [==============================] - 3s 546us/step - loss: 2.2123 - acc: 0.1950\n",
      "Epoch 5/200\n",
      "5435/5435 [==============================] - 3s 513us/step - loss: 2.1793 - acc: 0.2081\n",
      "Epoch 6/200\n",
      "5435/5435 [==============================] - 3s 602us/step - loss: 2.1613 - acc: 0.2075\n",
      "Epoch 7/200\n",
      "5435/5435 [==============================] - 3s 575us/step - loss: 2.1486 - acc: 0.2215\n",
      "Epoch 8/200\n",
      "5435/5435 [==============================] - 3s 542us/step - loss: 2.1145 - acc: 0.2280\n",
      "Epoch 9/200\n",
      "5435/5435 [==============================] - 3s 525us/step - loss: 2.0922 - acc: 0.2427\n",
      "Epoch 10/200\n",
      "5435/5435 [==============================] - 3s 582us/step - loss: 2.0952 - acc: 0.2355\n",
      "Epoch 11/200\n",
      "5435/5435 [==============================] - 3s 605us/step - loss: 2.0459 - acc: 0.2567\n",
      "Epoch 12/200\n",
      "5435/5435 [==============================] - 3s 507us/step - loss: 2.0500 - acc: 0.2613\n",
      "Epoch 13/200\n",
      "5435/5435 [==============================] - 3s 499us/step - loss: 2.0046 - acc: 0.2824\n",
      "Epoch 14/200\n",
      "5435/5435 [==============================] - 3s 489us/step - loss: 1.9919 - acc: 0.2837\n",
      "Epoch 15/200\n",
      "5435/5435 [==============================] - 2s 452us/step - loss: 1.9795 - acc: 0.2900\n",
      "Epoch 16/200\n",
      "5435/5435 [==============================] - 3s 515us/step - loss: 1.9771 - acc: 0.2868\n",
      "Epoch 17/200\n",
      "5435/5435 [==============================] - 3s 534us/step - loss: 1.9395 - acc: 0.3054\n",
      "Epoch 18/200\n",
      "5435/5435 [==============================] - 3s 496us/step - loss: 1.9217 - acc: 0.2995\n",
      "Epoch 19/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 1.8969 - acc: 0.3146\n",
      "Epoch 20/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 1.8875 - acc: 0.3198\n",
      "Epoch 21/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 1.8752 - acc: 0.3163\n",
      "Epoch 22/200\n",
      "5435/5435 [==============================] - 3s 491us/step - loss: 1.8704 - acc: 0.3319\n",
      "Epoch 23/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 1.8495 - acc: 0.3305\n",
      "Epoch 24/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 1.8346 - acc: 0.3373\n",
      "Epoch 25/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 1.8156 - acc: 0.3522\n",
      "Epoch 26/200\n",
      "5435/5435 [==============================] - 3s 482us/step - loss: 1.8096 - acc: 0.3454\n",
      "Epoch 27/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 1.7739 - acc: 0.3621\n",
      "Epoch 28/200\n",
      "5435/5435 [==============================] - 3s 485us/step - loss: 1.7682 - acc: 0.3652\n",
      "Epoch 29/200\n",
      "5435/5435 [==============================] - 3s 474us/step - loss: 1.7539 - acc: 0.3770\n",
      "Epoch 30/200\n",
      "5435/5435 [==============================] - 3s 483us/step - loss: 1.7282 - acc: 0.3877\n",
      "Epoch 31/200\n",
      "5435/5435 [==============================] - 3s 474us/step - loss: 1.7271 - acc: 0.3777\n",
      "Epoch 32/200\n",
      "5435/5435 [==============================] - 3s 483us/step - loss: 1.7003 - acc: 0.3934\n",
      "Epoch 33/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 1.6999 - acc: 0.3993\n",
      "Epoch 34/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 1.6836 - acc: 0.4031\n",
      "Epoch 35/200\n",
      "5435/5435 [==============================] - 3s 494us/step - loss: 1.6729 - acc: 0.4040\n",
      "Epoch 36/200\n",
      "5435/5435 [==============================] - 3s 484us/step - loss: 1.6669 - acc: 0.4160\n",
      "Epoch 37/200\n",
      "5435/5435 [==============================] - 3s 476us/step - loss: 1.6379 - acc: 0.4167\n",
      "Epoch 38/200\n",
      "5435/5435 [==============================] - 3s 476us/step - loss: 1.6272 - acc: 0.4278\n",
      "Epoch 39/200\n",
      "5435/5435 [==============================] - 3s 483us/step - loss: 1.6193 - acc: 0.4259\n",
      "Epoch 40/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 1.6039 - acc: 0.4328\n",
      "Epoch 41/200\n",
      "5435/5435 [==============================] - 3s 483us/step - loss: 1.5961 - acc: 0.4326\n",
      "Epoch 42/200\n",
      "5435/5435 [==============================] - 3s 478us/step - loss: 1.5829 - acc: 0.4320\n",
      "Epoch 43/200\n",
      "5435/5435 [==============================] - 3s 483us/step - loss: 1.5864 - acc: 0.4447\n",
      "Epoch 44/200\n",
      "5435/5435 [==============================] - 3s 477us/step - loss: 1.5636 - acc: 0.4438\n",
      "Epoch 45/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 1.5450 - acc: 0.4546\n",
      "Epoch 46/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 1.5438 - acc: 0.4517\n",
      "Epoch 47/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 1.5285 - acc: 0.4635\n",
      "Epoch 48/200\n",
      "5435/5435 [==============================] - 3s 480us/step - loss: 1.5057 - acc: 0.4629\n",
      "Epoch 49/200\n",
      "5435/5435 [==============================] - 3s 472us/step - loss: 1.5082 - acc: 0.4692\n",
      "Epoch 50/200\n",
      "5435/5435 [==============================] - 3s 477us/step - loss: 1.5049 - acc: 0.4646\n",
      "Epoch 51/200\n",
      "5435/5435 [==============================] - 3s 478us/step - loss: 1.4867 - acc: 0.4784\n",
      "Epoch 52/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 1.4616 - acc: 0.4769\n",
      "Epoch 53/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 1.4445 - acc: 0.4795\n",
      "Epoch 54/200\n",
      "5435/5435 [==============================] - 3s 480us/step - loss: 1.4599 - acc: 0.4806\n",
      "Epoch 55/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 1.4318 - acc: 0.4881\n",
      "Epoch 56/200\n",
      "5435/5435 [==============================] - 3s 480us/step - loss: 1.4314 - acc: 0.4914\n",
      "Epoch 57/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 1.4079 - acc: 0.5008\n",
      "Epoch 58/200\n",
      "5435/5435 [==============================] - 3s 480us/step - loss: 1.4032 - acc: 0.5019\n",
      "Epoch 59/200\n",
      "5435/5435 [==============================] - 3s 475us/step - loss: 1.4171 - acc: 0.4981\n",
      "Epoch 60/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 1.3894 - acc: 0.5082\n",
      "Epoch 61/200\n",
      "5435/5435 [==============================] - 3s 482us/step - loss: 1.3811 - acc: 0.5019\n",
      "Epoch 62/200\n",
      "5435/5435 [==============================] - 3s 476us/step - loss: 1.3747 - acc: 0.5109\n",
      "Epoch 63/200\n",
      "5435/5435 [==============================] - 3s 484us/step - loss: 1.3688 - acc: 0.5224\n",
      "Epoch 64/200\n",
      "5435/5435 [==============================] - 3s 532us/step - loss: 1.3583 - acc: 0.5218\n",
      "Epoch 65/200\n",
      "5435/5435 [==============================] - 3s 505us/step - loss: 1.3642 - acc: 0.5172\n",
      "Epoch 66/200\n",
      "5435/5435 [==============================] - 3s 465us/step - loss: 1.3366 - acc: 0.5216\n",
      "Epoch 67/200\n",
      "5435/5435 [==============================] - 2s 454us/step - loss: 1.3274 - acc: 0.5257\n",
      "Epoch 68/200\n",
      "5435/5435 [==============================] - 2s 447us/step - loss: 1.3089 - acc: 0.5356\n",
      "Epoch 69/200\n",
      "5435/5435 [==============================] - 2s 451us/step - loss: 1.3294 - acc: 0.5319\n",
      "Epoch 70/200\n",
      "5435/5435 [==============================] - 2s 448us/step - loss: 1.3083 - acc: 0.5303\n",
      "Epoch 71/200\n",
      "5435/5435 [==============================] - 2s 450us/step - loss: 1.3046 - acc: 0.5339\n",
      "Epoch 72/200\n",
      "5435/5435 [==============================] - 2s 456us/step - loss: 1.2874 - acc: 0.5411\n",
      "Epoch 73/200\n",
      "5435/5435 [==============================] - 2s 448us/step - loss: 1.2923 - acc: 0.5402\n",
      "Epoch 74/200\n",
      "5435/5435 [==============================] - 2s 452us/step - loss: 1.2770 - acc: 0.5474\n",
      "Epoch 75/200\n",
      "5435/5435 [==============================] - 2s 450us/step - loss: 1.2710 - acc: 0.5479\n",
      "Epoch 76/200\n",
      "5435/5435 [==============================] - 2s 454us/step - loss: 1.2463 - acc: 0.5534\n",
      "Epoch 77/200\n",
      "5435/5435 [==============================] - 2s 449us/step - loss: 1.2432 - acc: 0.5606\n",
      "Epoch 78/200\n",
      "5435/5435 [==============================] - 2s 449us/step - loss: 1.2553 - acc: 0.5516\n",
      "Epoch 79/200\n",
      "5435/5435 [==============================] - 3s 472us/step - loss: 1.2455 - acc: 0.5626\n",
      "Epoch 80/200\n",
      "5435/5435 [==============================] - 3s 530us/step - loss: 1.2267 - acc: 0.5555\n",
      "Epoch 81/200\n",
      "5435/5435 [==============================] - 3s 496us/step - loss: 1.2309 - acc: 0.5608\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - 3s 464us/step - loss: 1.2086 - acc: 0.5649\n",
      "Epoch 83/200\n",
      "5435/5435 [==============================] - 3s 625us/step - loss: 1.2208 - acc: 0.5715\n",
      "Epoch 84/200\n",
      "5435/5435 [==============================] - 3s 530us/step - loss: 1.2076 - acc: 0.5772\n",
      "Epoch 85/200\n",
      "5435/5435 [==============================] - 3s 575us/step - loss: 1.2048 - acc: 0.5639\n",
      "Epoch 86/200\n",
      "5435/5435 [==============================] - 3s 554us/step - loss: 1.1927 - acc: 0.5748\n",
      "Epoch 87/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 1.1966 - acc: 0.5768\n",
      "Epoch 88/200\n",
      "5435/5435 [==============================] - 2s 451us/step - loss: 1.1881 - acc: 0.5810\n",
      "Epoch 89/200\n",
      "5435/5435 [==============================] - 2s 454us/step - loss: 1.1777 - acc: 0.5776\n",
      "Epoch 90/200\n",
      "5435/5435 [==============================] - 3s 462us/step - loss: 1.1737 - acc: 0.5842\n",
      "Epoch 91/200\n",
      "5435/5435 [==============================] - 2s 451us/step - loss: 1.1564 - acc: 0.5939\n",
      "Epoch 92/200\n",
      "5435/5435 [==============================] - 3s 537us/step - loss: 1.1591 - acc: 0.5796\n",
      "Epoch 93/200\n",
      "5435/5435 [==============================] - 4s 663us/step - loss: 1.1437 - acc: 0.5967\n",
      "Epoch 94/200\n",
      "5435/5435 [==============================] - 4s 773us/step - loss: 1.1487 - acc: 0.5956\n",
      "Epoch 95/200\n",
      "5435/5435 [==============================] - 4s 772us/step - loss: 1.1324 - acc: 0.5950\n",
      "Epoch 96/200\n",
      "5435/5435 [==============================] - 4s 671us/step - loss: 1.1294 - acc: 0.5928\n",
      "Epoch 97/200\n",
      "5435/5435 [==============================] - 4s 683us/step - loss: 1.1228 - acc: 0.5989\n",
      "Epoch 98/200\n",
      "5435/5435 [==============================] - 4s 657us/step - loss: 1.1243 - acc: 0.6024\n",
      "Epoch 99/200\n",
      "5435/5435 [==============================] - 3s 590us/step - loss: 1.1171 - acc: 0.6040\n",
      "Epoch 100/200\n",
      "5435/5435 [==============================] - 3s 502us/step - loss: 1.1210 - acc: 0.6006\n",
      "Epoch 101/200\n",
      "5435/5435 [==============================] - 3s 508us/step - loss: 1.1215 - acc: 0.6013\n",
      "Epoch 102/200\n",
      "5435/5435 [==============================] - 3s 505us/step - loss: 1.1119 - acc: 0.5998\n",
      "Epoch 103/200\n",
      "5435/5435 [==============================] - 3s 566us/step - loss: 1.1125 - acc: 0.6042\n",
      "Epoch 104/200\n",
      "5435/5435 [==============================] - 3s 589us/step - loss: 1.0932 - acc: 0.6081\n",
      "Epoch 105/200\n",
      "5435/5435 [==============================] - 4s 662us/step - loss: 1.0991 - acc: 0.6123\n",
      "Epoch 106/200\n",
      "5435/5435 [==============================] - 3s 533us/step - loss: 1.0746 - acc: 0.6162\n",
      "Epoch 107/200\n",
      "5435/5435 [==============================] - 3s 564us/step - loss: 1.0615 - acc: 0.6178\n",
      "Epoch 108/200\n",
      "5435/5435 [==============================] - 3s 582us/step - loss: 1.0654 - acc: 0.6121\n",
      "Epoch 109/200\n",
      "5435/5435 [==============================] - 3s 517us/step - loss: 1.0582 - acc: 0.6171\n",
      "Epoch 110/200\n",
      "5435/5435 [==============================] - 3s 634us/step - loss: 1.0552 - acc: 0.6250\n",
      "Epoch 111/200\n",
      "5435/5435 [==============================] - 4s 679us/step - loss: 1.0404 - acc: 0.6388\n",
      "Epoch 112/200\n",
      "5435/5435 [==============================] - 3s 568us/step - loss: 1.0359 - acc: 0.6234\n",
      "Epoch 113/200\n",
      "5435/5435 [==============================] - 3s 499us/step - loss: 1.0419 - acc: 0.6276\n",
      "Epoch 114/200\n",
      "5435/5435 [==============================] - 3s 532us/step - loss: 1.0268 - acc: 0.6377\n",
      "Epoch 115/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 1.0238 - acc: 0.6351\n",
      "Epoch 116/200\n",
      "5435/5435 [==============================] - 3s 560us/step - loss: 1.0323 - acc: 0.6357\n",
      "Epoch 117/200\n",
      "5435/5435 [==============================] - 3s 538us/step - loss: 1.0238 - acc: 0.6396\n",
      "Epoch 118/200\n",
      "5435/5435 [==============================] - 3s 516us/step - loss: 1.0129 - acc: 0.6372\n",
      "Epoch 119/200\n",
      "5435/5435 [==============================] - 2s 452us/step - loss: 1.0111 - acc: 0.6412\n",
      "Epoch 120/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 1.0218 - acc: 0.6283\n",
      "Epoch 121/200\n",
      "5435/5435 [==============================] - 2s 452us/step - loss: 0.9900 - acc: 0.6495\n",
      "Epoch 122/200\n",
      "5435/5435 [==============================] - 3s 517us/step - loss: 0.9914 - acc: 0.6491\n",
      "Epoch 123/200\n",
      "5435/5435 [==============================] - 2s 454us/step - loss: 0.9914 - acc: 0.6460\n",
      "Epoch 124/200\n",
      "5435/5435 [==============================] - 3s 469us/step - loss: 0.9965 - acc: 0.6401\n",
      "Epoch 125/200\n",
      "5435/5435 [==============================] - 2s 455us/step - loss: 0.9871 - acc: 0.6381\n",
      "Epoch 126/200\n",
      "5435/5435 [==============================] - 3s 469us/step - loss: 0.9922 - acc: 0.6466\n",
      "Epoch 127/200\n",
      "5435/5435 [==============================] - 2s 455us/step - loss: 0.9682 - acc: 0.6592\n",
      "Epoch 128/200\n",
      "5435/5435 [==============================] - 3s 471us/step - loss: 0.9588 - acc: 0.6528\n",
      "Epoch 129/200\n",
      "5435/5435 [==============================] - 2s 450us/step - loss: 0.9657 - acc: 0.6598\n",
      "Epoch 130/200\n",
      "5435/5435 [==============================] - 3s 476us/step - loss: 0.9610 - acc: 0.6589\n",
      "Epoch 131/200\n",
      "5435/5435 [==============================] - 3s 496us/step - loss: 0.9768 - acc: 0.6499\n",
      "Epoch 132/200\n",
      "5435/5435 [==============================] - 3s 490us/step - loss: 0.9609 - acc: 0.6605\n",
      "Epoch 133/200\n",
      "5435/5435 [==============================] - 2s 447us/step - loss: 0.9584 - acc: 0.6557\n",
      "Epoch 134/200\n",
      "5435/5435 [==============================] - 3s 489us/step - loss: 0.9589 - acc: 0.6611\n",
      "Epoch 135/200\n",
      "5435/5435 [==============================] - 2s 448us/step - loss: 0.9549 - acc: 0.6633\n",
      "Epoch 136/200\n",
      "5435/5435 [==============================] - 3s 489us/step - loss: 0.9460 - acc: 0.6675\n",
      "Epoch 137/200\n",
      "5435/5435 [==============================] - 2s 448us/step - loss: 0.9401 - acc: 0.6574\n",
      "Epoch 138/200\n",
      "5435/5435 [==============================] - 3s 515us/step - loss: 0.9258 - acc: 0.6793\n",
      "Epoch 139/200\n",
      "5435/5435 [==============================] - 2s 447us/step - loss: 0.9260 - acc: 0.6775\n",
      "Epoch 140/200\n",
      "5435/5435 [==============================] - 3s 480us/step - loss: 0.9324 - acc: 0.6661\n",
      "Epoch 141/200\n",
      "5435/5435 [==============================] - 3s 500us/step - loss: 0.9208 - acc: 0.6695\n",
      "Epoch 142/200\n",
      "5435/5435 [==============================] - 3s 506us/step - loss: 0.9269 - acc: 0.6675\n",
      "Epoch 143/200\n",
      "5435/5435 [==============================] - 3s 467us/step - loss: 0.9180 - acc: 0.6753\n",
      "Epoch 144/200\n",
      "5435/5435 [==============================] - 2s 458us/step - loss: 0.9134 - acc: 0.6756\n",
      "Epoch 145/200\n",
      "5435/5435 [==============================] - 3s 467us/step - loss: 0.9117 - acc: 0.6776\n",
      "Epoch 146/200\n",
      "5435/5435 [==============================] - 2s 453us/step - loss: 0.9024 - acc: 0.6797\n",
      "Epoch 147/200\n",
      "5435/5435 [==============================] - 3s 467us/step - loss: 0.9018 - acc: 0.6775\n",
      "Epoch 148/200\n",
      "5435/5435 [==============================] - 2s 458us/step - loss: 0.8872 - acc: 0.6848\n",
      "Epoch 149/200\n",
      "5435/5435 [==============================] - 3s 478us/step - loss: 0.8992 - acc: 0.6797\n",
      "Epoch 150/200\n",
      "5435/5435 [==============================] - 2s 457us/step - loss: 0.8842 - acc: 0.6922\n",
      "Epoch 151/200\n",
      "5435/5435 [==============================] - 3s 469us/step - loss: 0.8831 - acc: 0.6870\n",
      "Epoch 152/200\n",
      "5435/5435 [==============================] - 2s 458us/step - loss: 0.8792 - acc: 0.6879\n",
      "Epoch 153/200\n",
      "5435/5435 [==============================] - 3s 464us/step - loss: 0.8817 - acc: 0.6850\n",
      "Epoch 154/200\n",
      "5435/5435 [==============================] - 3s 460us/step - loss: 0.8824 - acc: 0.6905\n",
      "Epoch 155/200\n",
      "5435/5435 [==============================] - 3s 462us/step - loss: 0.8779 - acc: 0.6861\n",
      "Epoch 156/200\n",
      "5435/5435 [==============================] - 2s 460us/step - loss: 0.8743 - acc: 0.6891\n",
      "Epoch 157/200\n",
      "5435/5435 [==============================] - 3s 463us/step - loss: 0.8715 - acc: 0.6887\n",
      "Epoch 158/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 0.8538 - acc: 0.6964\n",
      "Epoch 159/200\n",
      "5435/5435 [==============================] - 3s 501us/step - loss: 0.8646 - acc: 0.6942\n",
      "Epoch 160/200\n",
      "5435/5435 [==============================] - 2s 449us/step - loss: 0.8703 - acc: 0.6920\n",
      "Epoch 161/200\n",
      "5435/5435 [==============================] - 3s 516us/step - loss: 0.8543 - acc: 0.6937\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - 3s 518us/step - loss: 0.8527 - acc: 0.6986\n",
      "Epoch 163/200\n",
      "5435/5435 [==============================] - 3s 483us/step - loss: 0.8514 - acc: 0.6984\n",
      "Epoch 164/200\n",
      "5435/5435 [==============================] - 2s 448us/step - loss: 0.8274 - acc: 0.7054\n",
      "Epoch 165/200\n",
      "5435/5435 [==============================] - 3s 471us/step - loss: 0.8318 - acc: 0.7128\n",
      "Epoch 166/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 0.8257 - acc: 0.7060\n",
      "Epoch 167/200\n",
      "5435/5435 [==============================] - 2s 457us/step - loss: 0.8291 - acc: 0.7141\n",
      "Epoch 168/200\n",
      "5435/5435 [==============================] - 2s 449us/step - loss: 0.7993 - acc: 0.7148\n",
      "Epoch 169/200\n",
      "5435/5435 [==============================] - 2s 457us/step - loss: 0.8433 - acc: 0.7078\n",
      "Epoch 170/200\n",
      "5435/5435 [==============================] - 2s 450us/step - loss: 0.8067 - acc: 0.7216\n",
      "Epoch 171/200\n",
      "5435/5435 [==============================] - 2s 456us/step - loss: 0.8108 - acc: 0.7144\n",
      "Epoch 172/200\n",
      "5435/5435 [==============================] - 2s 453us/step - loss: 0.8109 - acc: 0.7185\n",
      "Epoch 173/200\n",
      "5435/5435 [==============================] - 2s 450us/step - loss: 0.8088 - acc: 0.7121\n",
      "Epoch 174/200\n",
      "5435/5435 [==============================] - 2s 457us/step - loss: 0.8065 - acc: 0.7181\n",
      "Epoch 175/200\n",
      "5435/5435 [==============================] - 2s 451us/step - loss: 0.8033 - acc: 0.7167\n",
      "Epoch 176/200\n",
      "5435/5435 [==============================] - 2s 457us/step - loss: 0.8146 - acc: 0.7091\n",
      "Epoch 177/200\n",
      "5435/5435 [==============================] - 2s 448us/step - loss: 0.7925 - acc: 0.7201\n",
      "Epoch 178/200\n",
      "5435/5435 [==============================] - 3s 487us/step - loss: 0.7994 - acc: 0.7207\n",
      "Epoch 179/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 0.7946 - acc: 0.7238\n",
      "Epoch 180/200\n",
      "5435/5435 [==============================] - 3s 489us/step - loss: 0.7915 - acc: 0.7247\n",
      "Epoch 181/200\n",
      "5435/5435 [==============================] - 3s 470us/step - loss: 0.7835 - acc: 0.7257\n",
      "Epoch 182/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 0.7782 - acc: 0.7270\n",
      "Epoch 183/200\n",
      "5435/5435 [==============================] - 3s 484us/step - loss: 0.7839 - acc: 0.7218\n",
      "Epoch 184/200\n",
      "5435/5435 [==============================] - 3s 499us/step - loss: 0.7660 - acc: 0.7338\n",
      "Epoch 185/200\n",
      "5435/5435 [==============================] - 3s 507us/step - loss: 0.7680 - acc: 0.7360\n",
      "Epoch 186/200\n",
      "5435/5435 [==============================] - 3s 493us/step - loss: 0.7720 - acc: 0.7286\n",
      "Epoch 187/200\n",
      "5435/5435 [==============================] - 2s 457us/step - loss: 0.7712 - acc: 0.7255\n",
      "Epoch 188/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 0.7630 - acc: 0.7321\n",
      "Epoch 189/200\n",
      "5435/5435 [==============================] - 3s 504us/step - loss: 0.7647 - acc: 0.7284\n",
      "Epoch 190/200\n",
      "5435/5435 [==============================] - 3s 488us/step - loss: 0.7762 - acc: 0.7310\n",
      "Epoch 191/200\n",
      "5435/5435 [==============================] - 3s 479us/step - loss: 0.7488 - acc: 0.7398\n",
      "Epoch 192/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 0.7489 - acc: 0.7374\n",
      "Epoch 193/200\n",
      "5435/5435 [==============================] - 3s 554us/step - loss: 0.7430 - acc: 0.7400\n",
      "Epoch 194/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 0.7543 - acc: 0.7409\n",
      "Epoch 195/200\n",
      "5435/5435 [==============================] - 3s 507us/step - loss: 0.7329 - acc: 0.7428\n",
      "Epoch 196/200\n",
      "5435/5435 [==============================] - 3s 489us/step - loss: 0.7514 - acc: 0.7376\n",
      "Epoch 197/200\n",
      "5435/5435 [==============================] - 3s 487us/step - loss: 0.7287 - acc: 0.7498\n",
      "Epoch 198/200\n",
      "5435/5435 [==============================] - 3s 553us/step - loss: 0.7372 - acc: 0.7450\n",
      "Epoch 199/200\n",
      "5435/5435 [==============================] - 3s 630us/step - loss: 0.7270 - acc: 0.7457\n",
      "Epoch 200/200\n",
      "5435/5435 [==============================] - 4s 703us/step - loss: 0.7270 - acc: 0.7514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', <keras.wrappers.scikit_learn.KerasClassifier object at 0x1c4edda048>)])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building a pipeline and fitting it\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=200, batch_size=1000, verbose=1, )))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "pipeline.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross-validating pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Row: 700\n",
      "Handling Row: 800\n",
      "Handling Row: 1600\n",
      "Handling Row: 1700\n",
      "Handling Row: 1900\n",
      "Handling Row: 2100\n",
      "Handling Row: 2400\n",
      "Handling Row: 2500\n",
      "Handling Row: 3000\n",
      "Handling Row: 3100\n",
      "Handling Row: 3300\n",
      "Handling Row: 3500\n",
      "Handling Row: 3600\n",
      "Handling Row: 4300\n",
      "Handling Row: 4600\n",
      "Handling Row: 4700\n",
      "Handling Row: 5000\n",
      "Handling Row: 5300\n",
      "Handling Row: 5500\n",
      "Handling Row: 5700\n",
      "Handling Row: 6000\n",
      "Handling Row: 6200\n",
      "Handling Row: 6400\n",
      "Handling Row: 6700\n",
      "Handling Row: 7300\n",
      "Handling Row: 8400\n",
      "Handling Row: 8600\n",
      "Handling Row: 8700\n"
     ]
    }
   ],
   "source": [
    "## creating a test dataframe\n",
    "\n",
    "test = pd.read_csv('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/test.csv')\n",
    "\n",
    "def feature_extractor_t(row):\n",
    "    # function to load files and extract features\n",
    "    file_name = os.path.join(os.path.abspath('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/'),\n",
    "                            'Test', str(row.ID) + '.wav')\n",
    "    if (row.ID%100 ==0):\n",
    "        print (\"Handling Row:\", row.ID)\n",
    "\n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        \n",
    "        #calculating MFCC feature\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=39).T,axis=0) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculating RMS feature for 400 frames\n",
    "        from madmom.audio.signal import FramedSignal\n",
    "        from madmom.audio.signal import root_mean_square\n",
    "\n",
    "        fs = FramedSignal(file_name, num_channels=1,\n",
    "                          num_frames=400)\n",
    "        ##rounding rms values to the nearest integer \n",
    "        rms = np.around(root_mean_square(fs))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculating ZCR feature\n",
    "        from librosa.feature import zero_crossing_rate\n",
    "        from librosa.core import load \n",
    "\n",
    "        ##padding with zeros to make a constant length of feature across all audios\n",
    "        zcr_padded = np.zeros((210,))\n",
    "        zcr = zero_crossing_rate(X, frame_length=2048, hop_length=441)\n",
    "        zcr_padded[:zcr[0].shape[0]] = zcr[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculating PSD feature\n",
    "        from scipy import signal\n",
    "        import matplotlib.pyplot as plt\n",
    "        from madmom.audio.signal import Signal\n",
    "\n",
    "        ##calculating PSD using Welch's method\n",
    "        s = Signal(file_name, num_channels=1)\n",
    "        f, P = signal.welch(s, 44100, nperseg=1024)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", e)\n",
    "        return None\n",
    "\n",
    "    features = np.concatenate ((mfccs, rms, zcr_padded, P))\n",
    "    \n",
    "\n",
    "    return features.tolist()\n",
    "\n",
    "new_test = test.apply(feature_extractor_t, axis=1)\n",
    "new_test.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3297/3297 [==============================] - 1s 162us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_t = np.array(new_test.tolist())\n",
    "\n",
    "lb.inverse_transform\n",
    "\n",
    "test['Class']=lb.inverse_transform(pipeline.predict(X_t))\n",
    "\n",
    "test.to_csv('predictions_1_new.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RMS as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extractor(row):\n",
    "    \n",
    "    # defining path to train files\n",
    "    file_name = os.path.join(os.path.abspath('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/'),\n",
    "                            'Train', str(row.ID) + '.wav')\n",
    "    if (row.ID%100 ==0):\n",
    "        print (\"Handling Row:\", row.ID)\n",
    "\n",
    "    \n",
    "    #enclosing all this in try-catch to handle exceptions and missing files\n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculating RMS feature for 400 frames\n",
    "        from madmom.audio.signal import FramedSignal\n",
    "        from madmom.audio.signal import root_mean_square\n",
    "\n",
    "        fs = FramedSignal(file_name, num_channels=1,\n",
    "                          num_frames=400)\n",
    "        ##rounding rms values to the nearest integer \n",
    "        rms = np.around(root_mean_square(fs))        \n",
    "        \n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None, None\n",
    "\n",
    "    features = rms\n",
    "    label = row.Class\n",
    "\n",
    "    return [features, label]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Row: 0\n",
      "Handling Row: 0\n",
      "Handling Row: 100\n",
      "Handling Row: 200\n",
      "Handling Row: 300\n",
      "Handling Row: 400\n",
      "Handling Row: 500\n",
      "Handling Row: 600\n",
      "Handling Row: 900\n",
      "Handling Row: 1000\n",
      "Handling Row: 1100\n",
      "Handling Row: 1200\n",
      "Handling Row: 1300\n",
      "Handling Row: 1400\n",
      "Handling Row: 1500\n",
      "Handling Row: 1800\n",
      "Handling Row: 2000\n",
      "Handling Row: 2200\n",
      "Handling Row: 2300\n",
      "Handling Row: 2600\n",
      "Handling Row: 2700\n",
      "Handling Row: 2800\n",
      "Handling Row: 2900\n",
      "Handling Row: 3200\n",
      "Handling Row: 3400\n",
      "Handling Row: 3700\n",
      "Handling Row: 3800\n",
      "Handling Row: 3900\n",
      "Handling Row: 4000\n",
      "Handling Row: 4100\n",
      "Handling Row: 4200\n",
      "Handling Row: 4400\n",
      "Handling Row: 4500\n",
      "Handling Row: 4800\n",
      "Handling Row: 4900\n",
      "Handling Row: 5100\n",
      "Handling Row: 5200\n",
      "Handling Row: 5400\n",
      "Handling Row: 5600\n",
      "Handling Row: 5800\n",
      "Handling Row: 5900\n",
      "Handling Row: 6100\n",
      "Handling Row: 6300\n",
      "Handling Row: 6500\n",
      "Handling Row: 6600\n",
      "Handling Row: 6800\n",
      "Handling Row: 6900\n",
      "Handling Row: 7000\n",
      "Handling Row: 7100\n",
      "Handling Row: 7200\n",
      "Handling Row: 7400\n",
      "Handling Row: 7500\n",
      "Handling Row: 7600\n",
      "Handling Row: 7700\n",
      "Handling Row: 7800\n",
      "Handling Row: 7900\n",
      "Handling Row: 8000\n",
      "Handling Row: 8100\n",
      "Handling Row: 8200\n",
      "Handling Row: 8300\n",
      "Handling Row: 8500\n"
     ]
    }
   ],
   "source": [
    "#applying feature_estractor on each row in train \n",
    "new_train = train.apply(feature_extractor, axis=1)\n",
    "new_train.columns = ['features', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [2230.0, 2711.0, 3081.0, 3126.0, 2970.0, 3176....\n",
       "1       [5906.0, 6513.0, 7260.0, 7742.0, 7488.0, 7246....\n",
       "2       [31.0, 32.0, 72.0, 72.0, 97.0, 97.0, 113.0, 10...\n",
       "3       [4277.0, 4822.0, 5387.0, 5252.0, 4918.0, 5078....\n",
       "4       [2028.0, 2490.0, 2755.0, 2442.0, 2822.0, 2554....\n",
       "5       [266.0, 304.0, 350.0, 359.0, 367.0, 356.0, 363...\n",
       "6       [462.0, 536.0, 604.0, 611.0, 601.0, 577.0, 557...\n",
       "7       [2097.0, 2484.0, 2763.0, 2844.0, 2731.0, 2726....\n",
       "8       [90.0, 107.0, 119.0, 7726.0, 9670.0, 10231.0, ...\n",
       "9       [189.0, 223.0, 231.0, 233.0, 219.0, 214.0, 214...\n",
       "10      [5457.0, 6106.0, 6391.0, 7235.0, 7726.0, 8298....\n",
       "11      [1437.0, 1750.0, 2173.0, 2267.0, 2254.0, 2126....\n",
       "12      [2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, ...\n",
       "13      [4679.0, 4980.0, 6560.0, 6523.0, 7333.0, 7390....\n",
       "14      [560.0, 668.0, 759.0, 779.0, 817.0, 782.0, 774...\n",
       "15      [1684.0, 2108.0, 2532.0, 2879.0, 2832.0, 2880....\n",
       "16      [2177.0, 2415.0, 2903.0, 2920.0, 3038.0, 3130....\n",
       "17      [714.0, 823.0, 864.0, 919.0, 847.0, 764.0, 791...\n",
       "18      [120.0, 127.0, 138.0, 137.0, 137.0, 130.0, 133...\n",
       "19      [1452.0, 1872.0, 2101.0, 2206.0, 2149.0, 2065....\n",
       "20      [6.0, 7.0, 8.0, 8.0, 9.0, 9.0, 9.0, 9.0, 9.0, ...\n",
       "21      [649.0, 715.0, 801.0, 864.0, 772.0, 718.0, 788...\n",
       "22      [666.0, 697.0, 722.0, 1129.0, 1446.0, 1718.0, ...\n",
       "23      [1507.0, 1702.0, 1820.0, 1644.0, 1414.0, 1170....\n",
       "24      [1747.0, 1793.0, 1809.0, 1673.0, 1536.0, 1639....\n",
       "25      [204.0, 239.0, 274.0, 272.0, 325.0, 342.0, 381...\n",
       "26      [2513.0, 2818.0, 3296.0, 3260.0, 3172.0, 3191....\n",
       "27      [116.0, 143.0, 158.0, 160.0, 168.0, 160.0, 151...\n",
       "28      [279.0, 340.0, 386.0, 1363.0, 1449.0, 1468.0, ...\n",
       "29      [401.0, 446.0, 494.0, 484.0, 481.0, 480.0, 481...\n",
       "                              ...                        \n",
       "5405    [414.0, 506.0, 562.0, 552.0, 534.0, 499.0, 477...\n",
       "5406    [2574.0, 2991.0, 3581.0, 3714.0, 3597.0, 3610....\n",
       "5407    [490.0, 573.0, 641.0, 657.0, 656.0, 678.0, 695...\n",
       "5408    [1559.0, 1745.0, 1992.0, 2186.0, 2106.0, 2042....\n",
       "5409    [795.0, 907.0, 997.0, 933.0, 900.0, 786.0, 823...\n",
       "5410    [484.0, 526.0, 733.0, 752.0, 682.0, 710.0, 635...\n",
       "5411    [2.0, 5.0, 9682.0, 12503.0, 13201.0, 15555.0, ...\n",
       "5412    [471.0, 524.0, 570.0, 592.0, 574.0, 582.0, 553...\n",
       "5413    [845.0, 975.0, 1067.0, 1015.0, 1172.0, 1203.0,...\n",
       "5414    [313.0, 335.0, 359.0, 389.0, 364.0, 411.0, 482...\n",
       "5415    [10596.0, 12811.0, 14044.0, 15030.0, 15476.0, ...\n",
       "5416    [264.0, 579.0, 927.0, 1352.0, 1483.0, 1521.0, ...\n",
       "5417    [1179.0, 1524.0, 1794.0, 1922.0, 2020.0, 2139....\n",
       "5418    [2449.0, 3413.0, 4294.0, 4918.0, 5162.0, 5946....\n",
       "5419    [3375.0, 4129.0, 4635.0, 4539.0, 4115.0, 3925....\n",
       "5420    [10.0, 201.0, 1337.0, 1917.0, 2018.0, 2148.0, ...\n",
       "5421    [325.0, 343.0, 373.0, 360.0, 314.0, 315.0, 309...\n",
       "5422    [1026.0, 1116.0, 1254.0, 1340.0, 1432.0, 1739....\n",
       "5423    [78.0, 366.0, 822.0, 1409.0, 2208.0, 2838.0, 3...\n",
       "5424    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "5425    [1870.0, 1918.0, 1930.0, 1184.0, 699.0, 786.0,...\n",
       "5426    [2027.0, 2399.0, 3015.0, 3280.0, 3300.0, 3294....\n",
       "5427    [152.0, 207.0, 498.0, 1094.0, 1628.0, 2492.0, ...\n",
       "5428    [1896.0, 2241.0, 3087.0, 2989.0, 3757.0, 3947....\n",
       "5429    [272.0, 319.0, 364.0, 366.0, 346.0, 347.0, 364...\n",
       "5430    [1943.0, 2243.0, 2682.0, 2669.0, 3130.0, 3408....\n",
       "5431    [12.0, 13.0, 14.0, 13.0, 13.0, 19.0, 78.0, 257...\n",
       "5432    [143.0, 212.0, 239.0, 255.0, 261.0, 258.0, 251...\n",
       "5433    [462.0, 682.0, 781.0, 850.0, 903.0, 961.0, 104...\n",
       "5434    [4071.0, 4891.0, 5530.0, 5843.0, 5616.0, 4817....\n",
       "Name: features, Length: 5435, dtype: object"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping rows for missing files\n",
    "new_train.dropna(inplace=True)\n",
    "new_train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2230.,  2711.,  3081., ...,  2230.,  2102.,  1856.],\n",
       "       [ 5906.,  6513.,  7260., ...,  8223.,  8203.,  8162.],\n",
       "       [   31.,    32.,    72., ...,  3696.,  3508.,  3087.],\n",
       "       ..., \n",
       "       [  143.,   212.,   239., ...,   229.,   231.,   227.],\n",
       "       [  462.,   682.,   781., ...,  1253.,  1286.,  1238.],\n",
       "       [ 4071.,  4891.,  5530., ...,  1723.,  1818.,  1681.]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing data for feeding into the model\n",
    "X = np.array(new_train.features.tolist())\n",
    "y = np.array(new_train.label.tolist())\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y = np_utils.to_categorical(lb.fit_transform(y))\n",
    "\n",
    "num_labels = y.shape[1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building the model architecture\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1024, input_shape=(400,), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1024, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(512, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd= SGD(momentum=0.2, lr=0.03)\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "    return model\n",
    "          \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5435/5435 [==============================] - 10s 2ms/step - loss: 2.8610 - acc: 0.1117\n",
      "Epoch 2/200\n",
      "5435/5435 [==============================] - 7s 1ms/step - loss: 2.4300 - acc: 0.1279\n",
      "Epoch 3/200\n",
      "5435/5435 [==============================] - 8s 1ms/step - loss: 2.3235 - acc: 0.1371\n",
      "Epoch 4/200\n",
      "5435/5435 [==============================] - 5s 971us/step - loss: 2.2930 - acc: 0.1419\n",
      "Epoch 5/200\n",
      "5435/5435 [==============================] - 4s 761us/step - loss: 2.2759 - acc: 0.1457\n",
      "Epoch 6/200\n",
      "5435/5435 [==============================] - 4s 736us/step - loss: 2.2456 - acc: 0.1557\n",
      "Epoch 7/200\n",
      "5435/5435 [==============================] - 4s 757us/step - loss: 2.2573 - acc: 0.1601\n",
      "Epoch 8/200\n",
      "5435/5435 [==============================] - 4s 782us/step - loss: 2.2264 - acc: 0.1647\n",
      "Epoch 9/200\n",
      "5435/5435 [==============================] - 5s 968us/step - loss: 2.2129 - acc: 0.1720\n",
      "Epoch 10/200\n",
      "5435/5435 [==============================] - 4s 754us/step - loss: 2.2082 - acc: 0.1661\n",
      "Epoch 11/200\n",
      "5435/5435 [==============================] - 4s 740us/step - loss: 2.1936 - acc: 0.1779\n",
      "Epoch 12/200\n",
      "5435/5435 [==============================] - 4s 798us/step - loss: 2.1822 - acc: 0.1928\n",
      "Epoch 13/200\n",
      "5435/5435 [==============================] - 4s 809us/step - loss: 2.1843 - acc: 0.1904\n",
      "Epoch 14/200\n",
      "5435/5435 [==============================] - 5s 872us/step - loss: 2.1697 - acc: 0.1890\n",
      "Epoch 15/200\n",
      "5435/5435 [==============================] - 4s 802us/step - loss: 2.1588 - acc: 0.1939\n",
      "Epoch 16/200\n",
      "5435/5435 [==============================] - 4s 722us/step - loss: 2.1512 - acc: 0.2000\n",
      "Epoch 17/200\n",
      "5435/5435 [==============================] - 4s 741us/step - loss: 2.1559 - acc: 0.1972\n",
      "Epoch 18/200\n",
      "5435/5435 [==============================] - 4s 735us/step - loss: 2.1458 - acc: 0.1989\n",
      "Epoch 19/200\n",
      "5435/5435 [==============================] - 4s 732us/step - loss: 2.1362 - acc: 0.2042\n",
      "Epoch 20/200\n",
      "5435/5435 [==============================] - 5s 836us/step - loss: 2.1362 - acc: 0.2006\n",
      "Epoch 21/200\n",
      "5435/5435 [==============================] - 4s 734us/step - loss: 2.1332 - acc: 0.1941\n",
      "Epoch 22/200\n",
      "5435/5435 [==============================] - 4s 806us/step - loss: 2.1201 - acc: 0.2033\n",
      "Epoch 23/200\n",
      "5435/5435 [==============================] - 4s 730us/step - loss: 2.1085 - acc: 0.2107\n",
      "Epoch 24/200\n",
      "5435/5435 [==============================] - 4s 737us/step - loss: 2.1113 - acc: 0.2081\n",
      "Epoch 25/200\n",
      "5435/5435 [==============================] - 4s 763us/step - loss: 2.1044 - acc: 0.2125\n",
      "Epoch 26/200\n",
      "5435/5435 [==============================] - 4s 729us/step - loss: 2.1006 - acc: 0.2173\n",
      "Epoch 27/200\n",
      "5435/5435 [==============================] - 4s 770us/step - loss: 2.0915 - acc: 0.2178\n",
      "Epoch 28/200\n",
      "5435/5435 [==============================] - 4s 740us/step - loss: 2.0943 - acc: 0.2156\n",
      "Epoch 29/200\n",
      "5435/5435 [==============================] - 4s 796us/step - loss: 2.0958 - acc: 0.2182\n",
      "Epoch 30/200\n",
      "5435/5435 [==============================] - 4s 741us/step - loss: 2.0874 - acc: 0.2167\n",
      "Epoch 31/200\n",
      "5435/5435 [==============================] - 4s 738us/step - loss: 2.0738 - acc: 0.2195\n",
      "Epoch 32/200\n",
      "5435/5435 [==============================] - 4s 735us/step - loss: 2.0784 - acc: 0.2173\n",
      "Epoch 33/200\n",
      "5435/5435 [==============================] - 4s 802us/step - loss: 2.0734 - acc: 0.2158\n",
      "Epoch 34/200\n",
      "5435/5435 [==============================] - 4s 732us/step - loss: 2.0652 - acc: 0.2221\n",
      "Epoch 35/200\n",
      "5435/5435 [==============================] - 4s 723us/step - loss: 2.0633 - acc: 0.2241\n",
      "Epoch 36/200\n",
      "5435/5435 [==============================] - 4s 747us/step - loss: 2.0596 - acc: 0.2252\n",
      "Epoch 37/200\n",
      "5435/5435 [==============================] - 4s 733us/step - loss: 2.0592 - acc: 0.2256\n",
      "Epoch 38/200\n",
      "5435/5435 [==============================] - 4s 737us/step - loss: 2.0530 - acc: 0.2274\n",
      "Epoch 39/200\n",
      "5435/5435 [==============================] - 4s 735us/step - loss: 2.0554 - acc: 0.2282\n",
      "Epoch 40/200\n",
      "5435/5435 [==============================] - 4s 734us/step - loss: 2.0416 - acc: 0.2344\n",
      "Epoch 41/200\n",
      "5435/5435 [==============================] - 4s 729us/step - loss: 2.0548 - acc: 0.2324\n",
      "Epoch 42/200\n",
      "5435/5435 [==============================] - 4s 768us/step - loss: 2.0378 - acc: 0.2348\n",
      "Epoch 43/200\n",
      "5435/5435 [==============================] - 4s 758us/step - loss: 2.0320 - acc: 0.2385\n",
      "Epoch 44/200\n",
      "5435/5435 [==============================] - 6s 1ms/step - loss: 2.0285 - acc: 0.2418\n",
      "Epoch 45/200\n",
      "5435/5435 [==============================] - 5s 938us/step - loss: 2.0309 - acc: 0.2304\n",
      "Epoch 46/200\n",
      "5435/5435 [==============================] - 5s 829us/step - loss: 2.0267 - acc: 0.2427\n",
      "Epoch 47/200\n",
      "5435/5435 [==============================] - 6s 1ms/step - loss: 2.0305 - acc: 0.2364\n",
      "Epoch 48/200\n",
      "5435/5435 [==============================] - 5s 946us/step - loss: 2.0234 - acc: 0.2386\n",
      "Epoch 49/200\n",
      "5435/5435 [==============================] - 5s 832us/step - loss: 2.0183 - acc: 0.2495\n",
      "Epoch 50/200\n",
      "5435/5435 [==============================] - 4s 776us/step - loss: 2.0195 - acc: 0.2442\n",
      "Epoch 51/200\n",
      "5435/5435 [==============================] - 5s 883us/step - loss: 2.0102 - acc: 0.2414\n",
      "Epoch 52/200\n",
      "5435/5435 [==============================] - 4s 792us/step - loss: 2.0056 - acc: 0.2493\n",
      "Epoch 53/200\n",
      "5435/5435 [==============================] - 5s 887us/step - loss: 2.0176 - acc: 0.2399\n",
      "Epoch 54/200\n",
      "5435/5435 [==============================] - 4s 815us/step - loss: 2.0128 - acc: 0.2447\n",
      "Epoch 55/200\n",
      "5435/5435 [==============================] - 4s 784us/step - loss: 2.0069 - acc: 0.2425\n",
      "Epoch 56/200\n",
      "5435/5435 [==============================] - 4s 811us/step - loss: 1.9987 - acc: 0.2456\n",
      "Epoch 57/200\n",
      "5435/5435 [==============================] - 4s 799us/step - loss: 1.9976 - acc: 0.2432\n",
      "Epoch 58/200\n",
      "5435/5435 [==============================] - 5s 849us/step - loss: 2.0015 - acc: 0.2530\n",
      "Epoch 59/200\n",
      "5435/5435 [==============================] - 4s 783us/step - loss: 2.0006 - acc: 0.2506\n",
      "Epoch 60/200\n",
      "5435/5435 [==============================] - 5s 880us/step - loss: 2.0021 - acc: 0.2502\n",
      "Epoch 61/200\n",
      "5435/5435 [==============================] - 4s 826us/step - loss: 1.9951 - acc: 0.2550\n",
      "Epoch 62/200\n",
      "5435/5435 [==============================] - 4s 807us/step - loss: 1.9897 - acc: 0.2530\n",
      "Epoch 63/200\n",
      "5435/5435 [==============================] - 4s 789us/step - loss: 1.9823 - acc: 0.2576\n",
      "Epoch 64/200\n",
      "5435/5435 [==============================] - 5s 970us/step - loss: 1.9800 - acc: 0.2550\n",
      "Epoch 65/200\n",
      "5435/5435 [==============================] - 5s 886us/step - loss: 1.9791 - acc: 0.2561\n",
      "Epoch 66/200\n",
      "5435/5435 [==============================] - 5s 858us/step - loss: 1.9748 - acc: 0.2596\n",
      "Epoch 67/200\n",
      "5435/5435 [==============================] - 5s 834us/step - loss: 1.9844 - acc: 0.2500\n",
      "Epoch 68/200\n",
      "5435/5435 [==============================] - 4s 793us/step - loss: 1.9711 - acc: 0.2548\n",
      "Epoch 69/200\n",
      "5435/5435 [==============================] - 4s 759us/step - loss: 1.9676 - acc: 0.2541\n",
      "Epoch 70/200\n",
      "5435/5435 [==============================] - 4s 750us/step - loss: 1.9694 - acc: 0.2646\n",
      "Epoch 71/200\n",
      "5435/5435 [==============================] - 5s 868us/step - loss: 1.9682 - acc: 0.2673\n",
      "Epoch 72/200\n",
      "5435/5435 [==============================] - 5s 869us/step - loss: 1.9665 - acc: 0.2624\n",
      "Epoch 73/200\n",
      "5435/5435 [==============================] - 5s 839us/step - loss: 1.9666 - acc: 0.2627\n",
      "Epoch 74/200\n",
      "5435/5435 [==============================] - 4s 754us/step - loss: 1.9616 - acc: 0.2565\n",
      "Epoch 75/200\n",
      "5435/5435 [==============================] - 4s 751us/step - loss: 1.9610 - acc: 0.2649\n",
      "Epoch 76/200\n",
      "5435/5435 [==============================] - 4s 739us/step - loss: 1.9567 - acc: 0.2651\n",
      "Epoch 77/200\n",
      "5435/5435 [==============================] - 4s 806us/step - loss: 1.9629 - acc: 0.2699\n",
      "Epoch 78/200\n",
      "5435/5435 [==============================] - 4s 749us/step - loss: 1.9593 - acc: 0.2672\n",
      "Epoch 79/200\n",
      "5435/5435 [==============================] - 5s 883us/step - loss: 1.9575 - acc: 0.2701\n",
      "Epoch 80/200\n",
      "5435/5435 [==============================] - 453s 83ms/step - loss: 1.9552 - acc: 0.2690\n",
      "Epoch 81/200\n",
      "5435/5435 [==============================] - 3s 550us/step - loss: 1.9511 - acc: 0.2615\n",
      "Epoch 82/200\n",
      "5435/5435 [==============================] - 2s 434us/step - loss: 1.9524 - acc: 0.2670\n",
      "Epoch 83/200\n",
      "5435/5435 [==============================] - 2s 401us/step - loss: 1.9505 - acc: 0.2769\n",
      "Epoch 84/200\n",
      "5435/5435 [==============================] - 3s 573us/step - loss: 1.9465 - acc: 0.2723\n",
      "Epoch 85/200\n",
      "5435/5435 [==============================] - 3s 516us/step - loss: 1.9410 - acc: 0.2705\n",
      "Epoch 86/200\n",
      "5435/5435 [==============================] - 2s 445us/step - loss: 1.9377 - acc: 0.2738\n",
      "Epoch 87/200\n",
      "5435/5435 [==============================] - 2s 403us/step - loss: 1.9434 - acc: 0.2699\n",
      "Epoch 88/200\n",
      "5435/5435 [==============================] - 2s 412us/step - loss: 1.9365 - acc: 0.2789\n",
      "Epoch 89/200\n",
      "5435/5435 [==============================] - 2s 400us/step - loss: 1.9341 - acc: 0.2802\n",
      "Epoch 90/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 1.9393 - acc: 0.2754\n",
      "Epoch 91/200\n",
      "5435/5435 [==============================] - 2s 421us/step - loss: 1.9358 - acc: 0.2832\n",
      "Epoch 92/200\n",
      "5435/5435 [==============================] - 2s 410us/step - loss: 1.9276 - acc: 0.2810\n",
      "Epoch 93/200\n",
      "5435/5435 [==============================] - 2s 424us/step - loss: 1.9379 - acc: 0.2839\n",
      "Epoch 94/200\n",
      "5435/5435 [==============================] - 2s 409us/step - loss: 1.9328 - acc: 0.2778\n",
      "Epoch 95/200\n",
      "5435/5435 [==============================] - 2s 412us/step - loss: 1.9268 - acc: 0.2697\n",
      "Epoch 96/200\n",
      "5435/5435 [==============================] - 2s 419us/step - loss: 1.9229 - acc: 0.2789\n",
      "Epoch 97/200\n",
      "5435/5435 [==============================] - 2s 437us/step - loss: 1.9211 - acc: 0.2808\n",
      "Epoch 98/200\n",
      "5435/5435 [==============================] - 2s 377us/step - loss: 1.9283 - acc: 0.2741\n",
      "Epoch 99/200\n",
      "5435/5435 [==============================] - 2s 379us/step - loss: 1.9165 - acc: 0.2852\n",
      "Epoch 100/200\n",
      "5435/5435 [==============================] - 2s 374us/step - loss: 1.9252 - acc: 0.2819\n",
      "Epoch 101/200\n",
      "5435/5435 [==============================] - 2s 388us/step - loss: 1.9188 - acc: 0.2821\n",
      "Epoch 102/200\n",
      "5435/5435 [==============================] - 2s 409us/step - loss: 1.9167 - acc: 0.2811\n",
      "Epoch 103/200\n",
      "5435/5435 [==============================] - 2s 376us/step - loss: 1.9205 - acc: 0.2789\n",
      "Epoch 104/200\n",
      "5435/5435 [==============================] - 2s 390us/step - loss: 1.9092 - acc: 0.2837\n",
      "Epoch 105/200\n",
      "5435/5435 [==============================] - 2s 384us/step - loss: 1.9116 - acc: 0.2889\n",
      "Epoch 106/200\n",
      "5435/5435 [==============================] - 2s 387us/step - loss: 1.9052 - acc: 0.2857\n",
      "Epoch 107/200\n",
      "5435/5435 [==============================] - 2s 378us/step - loss: 1.9103 - acc: 0.2830\n",
      "Epoch 108/200\n",
      "5435/5435 [==============================] - 2s 381us/step - loss: 1.9069 - acc: 0.2856\n",
      "Epoch 109/200\n",
      "5435/5435 [==============================] - 2s 381us/step - loss: 1.9008 - acc: 0.2863\n",
      "Epoch 110/200\n",
      "5435/5435 [==============================] - 2s 378us/step - loss: 1.9032 - acc: 0.2863\n",
      "Epoch 111/200\n",
      "5435/5435 [==============================] - 2s 378us/step - loss: 1.8976 - acc: 0.2804\n",
      "Epoch 112/200\n",
      "5435/5435 [==============================] - 2s 439us/step - loss: 1.9012 - acc: 0.2802\n",
      "Epoch 113/200\n",
      "5435/5435 [==============================] - 2s 374us/step - loss: 1.9017 - acc: 0.2876\n",
      "Epoch 114/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.8944 - acc: 0.2845\n",
      "Epoch 115/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.8988 - acc: 0.2876\n",
      "Epoch 116/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.9003 - acc: 0.2898\n",
      "Epoch 117/200\n",
      "5435/5435 [==============================] - 2s 385us/step - loss: 1.8940 - acc: 0.2887\n",
      "Epoch 118/200\n",
      "5435/5435 [==============================] - 2s 410us/step - loss: 1.8959 - acc: 0.2850\n",
      "Epoch 119/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.8903 - acc: 0.2872\n",
      "Epoch 120/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.8956 - acc: 0.2898\n",
      "Epoch 121/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.8860 - acc: 0.2896\n",
      "Epoch 122/200\n",
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.8951 - acc: 0.2913\n",
      "Epoch 123/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.8939 - acc: 0.2889\n",
      "Epoch 124/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.8820 - acc: 0.2968\n",
      "Epoch 125/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.8846 - acc: 0.2988\n",
      "Epoch 126/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.8876 - acc: 0.2955\n",
      "Epoch 127/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.8758 - acc: 0.3008\n",
      "Epoch 128/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.8870 - acc: 0.2964\n",
      "Epoch 129/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.8743 - acc: 0.2948\n",
      "Epoch 130/200\n",
      "5435/5435 [==============================] - 2s 390us/step - loss: 1.8885 - acc: 0.2911\n",
      "Epoch 131/200\n",
      "5435/5435 [==============================] - 2s 378us/step - loss: 1.8641 - acc: 0.2992\n",
      "Epoch 132/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.8776 - acc: 0.2896\n",
      "Epoch 133/200\n",
      "5435/5435 [==============================] - 2s 391us/step - loss: 1.8716 - acc: 0.2942\n",
      "Epoch 134/200\n",
      "5435/5435 [==============================] - 2s 404us/step - loss: 1.8692 - acc: 0.3025\n",
      "Epoch 135/200\n",
      "5435/5435 [==============================] - 2s 402us/step - loss: 1.8753 - acc: 0.2948\n",
      "Epoch 136/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.8700 - acc: 0.2959\n",
      "Epoch 137/200\n",
      "5435/5435 [==============================] - 2s 374us/step - loss: 1.8682 - acc: 0.2990\n",
      "Epoch 138/200\n",
      "5435/5435 [==============================] - 2s 359us/step - loss: 1.8616 - acc: 0.2990\n",
      "Epoch 139/200\n",
      "5435/5435 [==============================] - 2s 373us/step - loss: 1.8633 - acc: 0.2927\n",
      "Epoch 140/200\n",
      "5435/5435 [==============================] - 2s 369us/step - loss: 1.8627 - acc: 0.3003\n",
      "Epoch 141/200\n",
      "5435/5435 [==============================] - 2s 360us/step - loss: 1.8542 - acc: 0.3010\n",
      "Epoch 142/200\n",
      "5435/5435 [==============================] - 2s 375us/step - loss: 1.8616 - acc: 0.2971\n",
      "Epoch 143/200\n",
      "5435/5435 [==============================] - 2s 375us/step - loss: 1.8574 - acc: 0.3075\n",
      "Epoch 144/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.8589 - acc: 0.2966\n",
      "Epoch 145/200\n",
      "5435/5435 [==============================] - 2s 358us/step - loss: 1.8573 - acc: 0.3017\n",
      "Epoch 146/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.8577 - acc: 0.3084\n",
      "Epoch 147/200\n",
      "5435/5435 [==============================] - 2s 374us/step - loss: 1.8553 - acc: 0.3023\n",
      "Epoch 148/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.8518 - acc: 0.3030\n",
      "Epoch 149/200\n",
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.8549 - acc: 0.3086\n",
      "Epoch 150/200\n",
      "5435/5435 [==============================] - 2s 360us/step - loss: 1.8486 - acc: 0.3075\n",
      "Epoch 151/200\n",
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.8640 - acc: 0.3019\n",
      "Epoch 152/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.8546 - acc: 0.3058\n",
      "Epoch 153/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.8436 - acc: 0.3051\n",
      "Epoch 154/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.8490 - acc: 0.3124\n",
      "Epoch 155/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.8580 - acc: 0.2994\n",
      "Epoch 156/200\n",
      "5435/5435 [==============================] - 2s 359us/step - loss: 1.8394 - acc: 0.3098\n",
      "Epoch 157/200\n",
      "5435/5435 [==============================] - 2s 360us/step - loss: 1.8435 - acc: 0.3029\n",
      "Epoch 158/200\n",
      "5435/5435 [==============================] - 2s 370us/step - loss: 1.8357 - acc: 0.3093\n",
      "Epoch 159/200\n",
      "5435/5435 [==============================] - 2s 377us/step - loss: 1.8317 - acc: 0.3126\n",
      "Epoch 160/200\n",
      "5435/5435 [==============================] - 2s 358us/step - loss: 1.8390 - acc: 0.3122\n",
      "Epoch 161/200\n",
      "5435/5435 [==============================] - 2s 359us/step - loss: 1.8423 - acc: 0.3086\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - 2s 358us/step - loss: 1.8329 - acc: 0.3137\n",
      "Epoch 163/200\n",
      "5435/5435 [==============================] - 2s 357us/step - loss: 1.8372 - acc: 0.3102\n",
      "Epoch 164/200\n",
      "5435/5435 [==============================] - 2s 358us/step - loss: 1.8396 - acc: 0.3067\n",
      "Epoch 165/200\n",
      "5435/5435 [==============================] - 2s 357us/step - loss: 1.8310 - acc: 0.3091\n",
      "Epoch 166/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.8250 - acc: 0.3128\n",
      "Epoch 167/200\n",
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.8414 - acc: 0.3045\n",
      "Epoch 168/200\n",
      "5435/5435 [==============================] - 2s 359us/step - loss: 1.8222 - acc: 0.3200\n",
      "Epoch 169/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.8363 - acc: 0.3176\n",
      "Epoch 170/200\n",
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.8290 - acc: 0.3143\n",
      "Epoch 171/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.8200 - acc: 0.3097\n",
      "Epoch 172/200\n",
      "5435/5435 [==============================] - 2s 357us/step - loss: 1.8286 - acc: 0.3093\n",
      "Epoch 173/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.8383 - acc: 0.3073\n",
      "Epoch 174/200\n",
      "5435/5435 [==============================] - 2s 355us/step - loss: 1.8187 - acc: 0.3111\n",
      "Epoch 175/200\n",
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.8280 - acc: 0.3113\n",
      "Epoch 176/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.8179 - acc: 0.3080\n",
      "Epoch 177/200\n",
      "5435/5435 [==============================] - 2s 354us/step - loss: 1.8215 - acc: 0.3143\n",
      "Epoch 178/200\n",
      "5435/5435 [==============================] - 2s 358us/step - loss: 1.8291 - acc: 0.3111\n",
      "Epoch 179/200\n",
      "5435/5435 [==============================] - 2s 357us/step - loss: 1.8154 - acc: 0.3157\n",
      "Epoch 180/200\n",
      "5435/5435 [==============================] - 2s 360us/step - loss: 1.8134 - acc: 0.3133\n",
      "Epoch 181/200\n",
      "5435/5435 [==============================] - 2s 358us/step - loss: 1.8198 - acc: 0.3211\n",
      "Epoch 182/200\n",
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.8185 - acc: 0.3187\n",
      "Epoch 183/200\n",
      "5435/5435 [==============================] - 2s 356us/step - loss: 1.8180 - acc: 0.3109\n",
      "Epoch 184/200\n",
      "5435/5435 [==============================] - 2s 355us/step - loss: 1.8104 - acc: 0.3146\n",
      "Epoch 185/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.8096 - acc: 0.3146\n",
      "Epoch 186/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.8147 - acc: 0.3161\n",
      "Epoch 187/200\n",
      "5435/5435 [==============================] - 2s 356us/step - loss: 1.8139 - acc: 0.3139\n",
      "Epoch 188/200\n",
      "5435/5435 [==============================] - 2s 357us/step - loss: 1.8177 - acc: 0.3174\n",
      "Epoch 189/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.8134 - acc: 0.3179\n",
      "Epoch 190/200\n",
      "5435/5435 [==============================] - 2s 359us/step - loss: 1.8102 - acc: 0.3257\n",
      "Epoch 191/200\n",
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.8121 - acc: 0.3159\n",
      "Epoch 192/200\n",
      "5435/5435 [==============================] - 2s 360us/step - loss: 1.8045 - acc: 0.3078\n",
      "Epoch 193/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.7965 - acc: 0.3216\n",
      "Epoch 194/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.8004 - acc: 0.3271\n",
      "Epoch 195/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.7993 - acc: 0.3284\n",
      "Epoch 196/200\n",
      "5435/5435 [==============================] - 2s 360us/step - loss: 1.8092 - acc: 0.3144\n",
      "Epoch 197/200\n",
      "5435/5435 [==============================] - 2s 379us/step - loss: 1.7939 - acc: 0.3301\n",
      "Epoch 198/200\n",
      "5435/5435 [==============================] - 2s 358us/step - loss: 1.8050 - acc: 0.3225\n",
      "Epoch 199/200\n",
      "5435/5435 [==============================] - 2s 359us/step - loss: 1.7909 - acc: 0.3295\n",
      "Epoch 200/200\n",
      "5435/5435 [==============================] - 2s 360us/step - loss: 1.8076 - acc: 0.3190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', <keras.wrappers.scikit_learn.KerasClassifier object at 0x1c4e6faf60>)])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building a pipeline and fitting it\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=200, batch_size=1000, verbose=1, )))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "pipeline.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross-validating pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Row: 700\n",
      "Handling Row: 800\n",
      "Handling Row: 1600\n",
      "Handling Row: 1700\n",
      "Handling Row: 1900\n",
      "Handling Row: 2100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-004780c8f2c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mnew_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_extractor_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mnew_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4260\u001b[0m                         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4261\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4262\u001b[0;31m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[1;32m   4263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4316\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_agg_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4317\u001b[0m                     result = lib.reduce(values, func, axis=axis, dummy=dummy,\n\u001b[0;32m-> 4318\u001b[0;31m                                         labels=labels)\n\u001b[0m\u001b[1;32m   4319\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4320\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/reduce.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.reduce\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/reduce.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-004780c8f2c8>\u001b[0m in \u001b[0;36mfeature_extractor_t\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         fs = FramedSignal(file_name, num_channels=1,\n\u001b[0;32m---> 22\u001b[0;31m                           num_frames=400)\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m##rounding rms values to the nearest integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mrms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_mean_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/madmom/audio/signal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, signal, frame_size, hop_size, fps, origin, end, num_frames, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0;31m# try to instantiate a Signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m             \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSignal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;31m# save the signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/madmom/audio/signal.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, sample_rate, num_channels, start, stop, norm, gain, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m                                                 \u001b[0mnum_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                                                 \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                                                 dtype=dtype)\n\u001b[0m\u001b[1;32m    744\u001b[0m         \u001b[0;31m# cast as Signal if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/madmom/audio/signal.py\u001b[0m in \u001b[0;36mload_audio_file\u001b[0;34m(filename, sample_rate, num_channels, start, stop, dtype)\u001b[0m\n\u001b[1;32m    615\u001b[0m         return load_ffmpeg_file(filename, sample_rate=sample_rate,\n\u001b[1;32m    616\u001b[0m                                 \u001b[0mnum_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                                 stop=stop, dtype=dtype)\n\u001b[0m\u001b[1;32m    618\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;31m# ffmpeg is not present, try avconv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/madmom/audio/ffmpeg.py\u001b[0m in \u001b[0;36mload_ffmpeg_file\u001b[0;34m(filename, sample_rate, num_channels, start, stop, dtype, cmd_decode, cmd_probe)\u001b[0m\n\u001b[1;32m    409\u001b[0m                                             \u001b[0mnum_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                                             \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                                             cmd=cmd_decode),\n\u001b[0m\u001b[1;32m    412\u001b[0m                            dtype=dtype)\n\u001b[1;32m    413\u001b[0m     \u001b[0;31m# get the needed information from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/madmom/audio/ffmpeg.py\u001b[0m in \u001b[0;36mdecode_to_memory\u001b[0;34m(infile, fmt, sample_rate, num_channels, skip, max_len, cmd)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    828\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## creating a test dataframe\n",
    "\n",
    "test = pd.read_csv('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/test.csv')\n",
    "\n",
    "def feature_extractor_t(row):\n",
    "    # function to load files and extract features\n",
    "    file_name = os.path.join(os.path.abspath('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/'),\n",
    "                            'Test', str(row.ID) + '.wav')\n",
    "    if (row.ID%100 ==0):\n",
    "        print (\"Handling Row:\", row.ID)\n",
    "\n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        \n",
    "\n",
    "        \n",
    "        #calculating RMS feature for 400 frames\n",
    "        from madmom.audio.signal import FramedSignal\n",
    "        from madmom.audio.signal import root_mean_square\n",
    "\n",
    "        fs = FramedSignal(file_name, num_channels=1,\n",
    "                          num_frames=400)\n",
    "        ##rounding rms values to the nearest integer \n",
    "        rms = np.around(root_mean_square(fs))\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "\n",
    "    features = rms\n",
    "    \n",
    "\n",
    "    return features.tolist()\n",
    "\n",
    "new_test = test.apply(feature_extractor_t, axis=1)\n",
    "new_test.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3297/3297 [==============================] - 1s 217us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_t = np.array(new_test.tolist())\n",
    "\n",
    "lb.inverse_transform\n",
    "\n",
    "test['Class']=lb.inverse_transform(pipeline.predict(X_t))\n",
    "\n",
    "test.to_csv('predictions_2.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ZCR feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extractor(row):\n",
    "    \n",
    "    # defining path to train files\n",
    "    file_name = os.path.join(os.path.abspath('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/'),\n",
    "                            'Train', str(row.ID) + '.wav')\n",
    "    if (row.ID%100 ==0):\n",
    "        print (\"Handling Row:\", row.ID)\n",
    "    \n",
    "    #enclosing all this in try-catch to handle exceptions and missing files\n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "\n",
    "        \n",
    "        \n",
    "        #calculating ZCR feature\n",
    "        from librosa.feature import zero_crossing_rate\n",
    "        from librosa.core import load \n",
    "\n",
    "        ##padding with zeros to make a constant length of feature across all audios\n",
    "        zcr_padded = np.zeros((210,))\n",
    "        zcr = zero_crossing_rate(X, frame_length=2048, hop_length=441)\n",
    "        zcr_padded[:zcr[0].shape[0]] = zcr[0]\n",
    "        zcr_padded\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None, None\n",
    "\n",
    "    features = zcr_padded\n",
    "    label = row.Class\n",
    "\n",
    "    return [features.tolist(), label]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Row: 0\n",
      "Handling Row: 0\n",
      "Handling Row: 100\n",
      "Handling Row: 200\n",
      "Handling Row: 300\n",
      "Handling Row: 400\n",
      "Handling Row: 500\n",
      "Handling Row: 600\n",
      "Handling Row: 900\n",
      "Handling Row: 1000\n",
      "Handling Row: 1100\n",
      "Handling Row: 1200\n",
      "Handling Row: 1300\n",
      "Handling Row: 1400\n",
      "Handling Row: 1500\n",
      "Handling Row: 1800\n",
      "Handling Row: 2000\n",
      "Handling Row: 2200\n",
      "Handling Row: 2300\n",
      "Handling Row: 2600\n",
      "Handling Row: 2700\n",
      "Handling Row: 2800\n",
      "Handling Row: 2900\n",
      "Handling Row: 3200\n",
      "Handling Row: 3400\n",
      "Handling Row: 3700\n",
      "Handling Row: 3800\n",
      "Handling Row: 3900\n",
      "Handling Row: 4000\n",
      "Handling Row: 4100\n",
      "Handling Row: 4200\n",
      "Handling Row: 4400\n",
      "Handling Row: 4500\n",
      "Handling Row: 4800\n",
      "Handling Row: 4900\n",
      "Handling Row: 5100\n",
      "Handling Row: 5200\n",
      "Handling Row: 5400\n",
      "Handling Row: 5600\n",
      "Handling Row: 5800\n",
      "Handling Row: 5900\n",
      "Handling Row: 6100\n",
      "Handling Row: 6300\n",
      "Handling Row: 6500\n",
      "Handling Row: 6600\n",
      "Handling Row: 6800\n",
      "Handling Row: 6900\n",
      "Handling Row: 7000\n",
      "Handling Row: 7100\n",
      "Handling Row: 7200\n",
      "Handling Row: 7400\n",
      "Handling Row: 7500\n",
      "Handling Row: 7600\n",
      "Handling Row: 7700\n",
      "Handling Row: 7800\n",
      "Handling Row: 7900\n",
      "Handling Row: 8000\n",
      "Handling Row: 8100\n",
      "Handling Row: 8200\n",
      "Handling Row: 8300\n",
      "Handling Row: 8500\n"
     ]
    }
   ],
   "source": [
    "#applying feature_estractor on each row in train \n",
    "new_train = train.apply(feature_extractor, axis=1)\n",
    "new_train.columns = ['features', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5435, 2)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping rows for missing files\n",
    "new_train.dropna(inplace=True)\n",
    "new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04785156,  0.07226562,  0.09570312,  0.10742188,  0.10888672,\n",
       "        0.11083984,  0.10888672,  0.10742188,  0.10498047,  0.10888672,\n",
       "        0.10693359,  0.10058594,  0.09667969,  0.09570312,  0.09082031,\n",
       "        0.09033203,  0.09033203,  0.09033203,  0.08984375,  0.09521484,\n",
       "        0.09570312,  0.09960938,  0.09912109,  0.10058594,  0.10009766,\n",
       "        0.10058594,  0.10302734,  0.09716797,  0.09912109,  0.10205078,\n",
       "        0.09326172,  0.08837891,  0.08837891,  0.07666016,  0.07470703,\n",
       "        0.07324219,  0.05615234,  0.06103516,  0.06640625,  0.06396484,\n",
       "        0.07080078,  0.07861328,  0.07617188,  0.07568359,  0.08691406,\n",
       "        0.08886719,  0.08935547,  0.09228516,  0.08984375,  0.08300781,\n",
       "        0.08935547,  0.08789062,  0.08984375,  0.08691406,  0.09375   ,\n",
       "        0.09716797,  0.09326172,  0.09423828,  0.09277344,  0.09179688,\n",
       "        0.09130859,  0.09667969,  0.09423828,  0.09228516,  0.0859375 ,\n",
       "        0.0859375 ,  0.07568359,  0.08056641,  0.08837891,  0.08251953,\n",
       "        0.08789062,  0.08886719,  0.08496094,  0.08251953,  0.07666016,\n",
       "        0.07080078,  0.07568359,  0.07421875,  0.08398438,  0.09326172,\n",
       "        0.09716797,  0.09765625,  0.08837891,  0.08544922,  0.08398438,\n",
       "        0.08984375,  0.08789062,  0.09814453,  0.09521484,  0.08886719,\n",
       "        0.09277344,  0.08984375,  0.08935547,  0.08984375,  0.08837891,\n",
       "        0.08837891,  0.07910156,  0.07910156,  0.08056641,  0.07324219,\n",
       "        0.08007812,  0.08349609,  0.08056641,  0.08300781,  0.08105469,\n",
       "        0.08007812,  0.08300781,  0.08251953,  0.09179688,  0.09277344,\n",
       "        0.08789062,  0.08691406,  0.08398438,  0.09082031,  0.08642578,\n",
       "        0.08984375,  0.08837891,  0.08837891,  0.08984375,  0.07910156,\n",
       "        0.07470703,  0.07666016,  0.07568359,  0.07910156,  0.08496094,\n",
       "        0.08642578,  0.09130859,  0.08886719,  0.09521484,  0.09130859,\n",
       "        0.08544922,  0.08056641,  0.07861328,  0.07324219,  0.07617188,\n",
       "        0.08007812,  0.07714844,  0.07519531,  0.078125  ,  0.07861328,\n",
       "        0.08300781,  0.08691406,  0.08251953,  0.08154297,  0.08251953,\n",
       "        0.08154297,  0.08789062,  0.08544922,  0.08496094,  0.08740234,\n",
       "        0.09033203,  0.09179688,  0.09326172,  0.08886719,  0.08740234,\n",
       "        0.08251953,  0.0859375 ,  0.08789062,  0.09033203,  0.09179688,\n",
       "        0.09130859,  0.09667969,  0.09521484,  0.08740234,  0.07617188,\n",
       "        0.07519531,  0.07714844,  0.07910156,  0.08007812,  0.078125  ,\n",
       "        0.07910156,  0.07519531,  0.07666016,  0.07275391,  0.07519531,\n",
       "        0.07080078,  0.07617188,  0.07763672,  0.078125  ,  0.078125  ,\n",
       "        0.08154297,  0.07275391,  0.07128906,  0.07421875,  0.07617188,\n",
       "        0.07763672,  0.08740234,  0.08447266,  0.07421875,  0.06738281,\n",
       "        0.07373047,  0.07373047,  0.07666016,  0.08056641,  0.08007812,\n",
       "        0.07714844,  0.08300781,  0.09033203,  0.08789062,  0.07177734,\n",
       "        0.05566406,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing data for feeding into the model\n",
    "X = np.array(new_train.features.tolist())\n",
    "y = np.array(new_train.label.tolist())\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y = np_utils.to_categorical(lb.fit_transform(y))\n",
    "\n",
    "num_labels = y.shape[1]\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building the model architecture\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1024, input_shape=(210,), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1024, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(512, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd= SGD(momentum=0.2, lr=0.03)\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "    return model\n",
    "          \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5435/5435 [==============================] - 3s 494us/step - loss: 2.4551 - acc: 0.1338\n",
      "Epoch 2/200\n",
      "5435/5435 [==============================] - 2s 381us/step - loss: 2.2772 - acc: 0.1634\n",
      "Epoch 3/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 2.2137 - acc: 0.1820\n",
      "Epoch 4/200\n",
      "5435/5435 [==============================] - 2s 341us/step - loss: 2.2061 - acc: 0.1700\n",
      "Epoch 5/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 2.1785 - acc: 0.1856\n",
      "Epoch 6/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 2.1727 - acc: 0.1869\n",
      "Epoch 7/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 2.1435 - acc: 0.2057\n",
      "Epoch 8/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 2.1413 - acc: 0.2033\n",
      "Epoch 9/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 2.1367 - acc: 0.1928\n",
      "Epoch 10/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 2.1201 - acc: 0.1965\n",
      "Epoch 11/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 2.1177 - acc: 0.2070\n",
      "Epoch 12/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 2.1060 - acc: 0.2096\n",
      "Epoch 13/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 2.0916 - acc: 0.2149\n",
      "Epoch 14/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 2.0925 - acc: 0.2112\n",
      "Epoch 15/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 2.0830 - acc: 0.2230\n",
      "Epoch 16/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 2.0904 - acc: 0.2144\n",
      "Epoch 17/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 2.0741 - acc: 0.2191\n",
      "Epoch 18/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 2.0765 - acc: 0.2228\n",
      "Epoch 19/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 2.0671 - acc: 0.2164\n",
      "Epoch 20/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 2.0495 - acc: 0.2258\n",
      "Epoch 21/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 2.0368 - acc: 0.2298\n",
      "Epoch 22/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 2.0434 - acc: 0.2272\n",
      "Epoch 23/200\n",
      "5435/5435 [==============================] - 2s 335us/step - loss: 2.0417 - acc: 0.2248\n",
      "Epoch 24/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 2.0276 - acc: 0.2362\n",
      "Epoch 25/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 2.0338 - acc: 0.2351\n",
      "Epoch 26/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 2.0178 - acc: 0.2374\n",
      "Epoch 27/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 2.0031 - acc: 0.2478\n",
      "Epoch 28/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 2.0139 - acc: 0.2388\n",
      "Epoch 29/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 2.0090 - acc: 0.2432\n",
      "Epoch 30/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.9998 - acc: 0.2502\n",
      "Epoch 31/200\n",
      "5435/5435 [==============================] - 2s 341us/step - loss: 1.9955 - acc: 0.2543\n",
      "Epoch 32/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 2.0020 - acc: 0.2561\n",
      "Epoch 33/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.9906 - acc: 0.2508\n",
      "Epoch 34/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 1.9907 - acc: 0.2526\n",
      "Epoch 35/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.9795 - acc: 0.2583\n",
      "Epoch 36/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.9730 - acc: 0.2651\n",
      "Epoch 37/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.9740 - acc: 0.2524\n",
      "Epoch 38/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.9694 - acc: 0.2546\n",
      "Epoch 39/200\n",
      "5435/5435 [==============================] - 2s 341us/step - loss: 1.9698 - acc: 0.2583\n",
      "Epoch 40/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 1.9692 - acc: 0.2646\n",
      "Epoch 41/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.9640 - acc: 0.2675\n",
      "Epoch 42/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.9609 - acc: 0.2675\n",
      "Epoch 43/200\n",
      "5435/5435 [==============================] - 2s 351us/step - loss: 1.9622 - acc: 0.2611\n",
      "Epoch 44/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 1.9564 - acc: 0.2679\n",
      "Epoch 45/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.9606 - acc: 0.2631\n",
      "Epoch 46/200\n",
      "5435/5435 [==============================] - 2s 346us/step - loss: 1.9482 - acc: 0.2686\n",
      "Epoch 47/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 1.9481 - acc: 0.2602\n",
      "Epoch 48/200\n",
      "5435/5435 [==============================] - 2s 345us/step - loss: 1.9441 - acc: 0.2690\n",
      "Epoch 49/200\n",
      "5435/5435 [==============================] - 2s 351us/step - loss: 1.9408 - acc: 0.2778\n",
      "Epoch 50/200\n",
      "5435/5435 [==============================] - 2s 345us/step - loss: 1.9392 - acc: 0.2718\n",
      "Epoch 51/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 1.9447 - acc: 0.2727\n",
      "Epoch 52/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 1.9400 - acc: 0.2822\n",
      "Epoch 53/200\n",
      "5435/5435 [==============================] - 2s 350us/step - loss: 1.9300 - acc: 0.2780\n",
      "Epoch 54/200\n",
      "5435/5435 [==============================] - 2s 352us/step - loss: 1.9333 - acc: 0.2846\n",
      "Epoch 55/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 1.9330 - acc: 0.2791\n",
      "Epoch 56/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 1.9307 - acc: 0.2776\n",
      "Epoch 57/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.9241 - acc: 0.2865\n",
      "Epoch 58/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.9289 - acc: 0.2764\n",
      "Epoch 59/200\n",
      "5435/5435 [==============================] - 2s 346us/step - loss: 1.9189 - acc: 0.2764\n",
      "Epoch 60/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.9243 - acc: 0.2835\n",
      "Epoch 61/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.9146 - acc: 0.2909\n",
      "Epoch 62/200\n",
      "5435/5435 [==============================] - 2s 335us/step - loss: 1.9149 - acc: 0.2850\n",
      "Epoch 63/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.9071 - acc: 0.2960\n",
      "Epoch 64/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.9132 - acc: 0.2826\n",
      "Epoch 65/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 1.9174 - acc: 0.2843\n",
      "Epoch 66/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.9034 - acc: 0.2881\n",
      "Epoch 67/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.9129 - acc: 0.2846\n",
      "Epoch 68/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.9036 - acc: 0.2810\n",
      "Epoch 69/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.9026 - acc: 0.2922\n",
      "Epoch 70/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.9099 - acc: 0.2822\n",
      "Epoch 71/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.9082 - acc: 0.2891\n",
      "Epoch 72/200\n",
      "5435/5435 [==============================] - 2s 335us/step - loss: 1.9032 - acc: 0.2911\n",
      "Epoch 73/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.8936 - acc: 0.2898\n",
      "Epoch 74/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.8896 - acc: 0.2856\n",
      "Epoch 75/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8868 - acc: 0.2959\n",
      "Epoch 76/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8888 - acc: 0.2949\n",
      "Epoch 77/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 1.8959 - acc: 0.2927\n",
      "Epoch 78/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8894 - acc: 0.2966\n",
      "Epoch 79/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.8845 - acc: 0.2914\n",
      "Epoch 80/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 1.8848 - acc: 0.2959\n",
      "Epoch 81/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8810 - acc: 0.2964\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - 2s 376us/step - loss: 1.8907 - acc: 0.2898\n",
      "Epoch 83/200\n",
      "5435/5435 [==============================] - 2s 452us/step - loss: 1.8858 - acc: 0.2997\n",
      "Epoch 84/200\n",
      "5435/5435 [==============================] - 2s 388us/step - loss: 1.8814 - acc: 0.2929\n",
      "Epoch 85/200\n",
      "5435/5435 [==============================] - 2s 387us/step - loss: 1.8798 - acc: 0.3016\n",
      "Epoch 86/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.8816 - acc: 0.2995\n",
      "Epoch 87/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.8784 - acc: 0.3005\n",
      "Epoch 88/200\n",
      "5435/5435 [==============================] - 2s 341us/step - loss: 1.8734 - acc: 0.2994\n",
      "Epoch 89/200\n",
      "5435/5435 [==============================] - 2s 334us/step - loss: 1.8754 - acc: 0.2983\n",
      "Epoch 90/200\n",
      "5435/5435 [==============================] - 2s 351us/step - loss: 1.8685 - acc: 0.3032\n",
      "Epoch 91/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 1.8669 - acc: 0.2949\n",
      "Epoch 92/200\n",
      "5435/5435 [==============================] - 2s 341us/step - loss: 1.8778 - acc: 0.2900\n",
      "Epoch 93/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 1.8686 - acc: 0.2937\n",
      "Epoch 94/200\n",
      "5435/5435 [==============================] - 2s 375us/step - loss: 1.8706 - acc: 0.2973\n",
      "Epoch 95/200\n",
      "5435/5435 [==============================] - 2s 436us/step - loss: 1.8685 - acc: 0.3040\n",
      "Epoch 96/200\n",
      "5435/5435 [==============================] - 2s 376us/step - loss: 1.8612 - acc: 0.3084\n",
      "Epoch 97/200\n",
      "5435/5435 [==============================] - 2s 385us/step - loss: 1.8607 - acc: 0.3017\n",
      "Epoch 98/200\n",
      "5435/5435 [==============================] - 2s 401us/step - loss: 1.8654 - acc: 0.2986\n",
      "Epoch 99/200\n",
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.8618 - acc: 0.3038\n",
      "Epoch 100/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.8611 - acc: 0.3003\n",
      "Epoch 101/200\n",
      "5435/5435 [==============================] - 2s 393us/step - loss: 1.8593 - acc: 0.2966\n",
      "Epoch 102/200\n",
      "5435/5435 [==============================] - 2s 350us/step - loss: 1.8528 - acc: 0.3063\n",
      "Epoch 103/200\n",
      "5435/5435 [==============================] - 2s 341us/step - loss: 1.8582 - acc: 0.3041\n",
      "Epoch 104/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 1.8610 - acc: 0.3049\n",
      "Epoch 105/200\n",
      "5435/5435 [==============================] - 2s 358us/step - loss: 1.8520 - acc: 0.3034\n",
      "Epoch 106/200\n",
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.8505 - acc: 0.3025\n",
      "Epoch 107/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.8512 - acc: 0.3021\n",
      "Epoch 108/200\n",
      "5435/5435 [==============================] - 2s 347us/step - loss: 1.8560 - acc: 0.3051\n",
      "Epoch 109/200\n",
      "5435/5435 [==============================] - 2s 345us/step - loss: 1.8566 - acc: 0.2999\n",
      "Epoch 110/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 1.8486 - acc: 0.3038\n",
      "Epoch 111/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.8428 - acc: 0.3082\n",
      "Epoch 112/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.8430 - acc: 0.3067\n",
      "Epoch 113/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.8431 - acc: 0.3091\n",
      "Epoch 114/200\n",
      "5435/5435 [==============================] - 2s 334us/step - loss: 1.8428 - acc: 0.3058\n",
      "Epoch 115/200\n",
      "5435/5435 [==============================] - 2s 360us/step - loss: 1.8419 - acc: 0.3058\n",
      "Epoch 116/200\n",
      "5435/5435 [==============================] - 2s 348us/step - loss: 1.8404 - acc: 0.3043\n",
      "Epoch 117/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8346 - acc: 0.3087\n",
      "Epoch 118/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.8437 - acc: 0.3080\n",
      "Epoch 119/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8472 - acc: 0.3100\n",
      "Epoch 120/200\n",
      "5435/5435 [==============================] - 2s 351us/step - loss: 1.8353 - acc: 0.3058\n",
      "Epoch 121/200\n",
      "5435/5435 [==============================] - 2s 417us/step - loss: 1.8325 - acc: 0.3189\n",
      "Epoch 122/200\n",
      "5435/5435 [==============================] - 2s 377us/step - loss: 1.8285 - acc: 0.3078\n",
      "Epoch 123/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.8377 - acc: 0.3161\n",
      "Epoch 124/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8314 - acc: 0.3058\n",
      "Epoch 125/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.8328 - acc: 0.3073\n",
      "Epoch 126/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8374 - acc: 0.3115\n",
      "Epoch 127/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.8281 - acc: 0.3091\n",
      "Epoch 128/200\n",
      "5435/5435 [==============================] - 2s 341us/step - loss: 1.8313 - acc: 0.3185\n",
      "Epoch 129/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.8252 - acc: 0.3192\n",
      "Epoch 130/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8263 - acc: 0.3089\n",
      "Epoch 131/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.8268 - acc: 0.3113\n",
      "Epoch 132/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.8270 - acc: 0.3091\n",
      "Epoch 133/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.8185 - acc: 0.3168\n",
      "Epoch 134/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.8240 - acc: 0.3135\n",
      "Epoch 135/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.8194 - acc: 0.3168\n",
      "Epoch 136/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.8217 - acc: 0.3172\n",
      "Epoch 137/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.8084 - acc: 0.3227\n",
      "Epoch 138/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.8221 - acc: 0.3095\n",
      "Epoch 139/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8173 - acc: 0.3086\n",
      "Epoch 140/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.8130 - acc: 0.3170\n",
      "Epoch 141/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8142 - acc: 0.3155\n",
      "Epoch 142/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.8226 - acc: 0.3205\n",
      "Epoch 143/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.8166 - acc: 0.3143\n",
      "Epoch 144/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.8132 - acc: 0.3144\n",
      "Epoch 145/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.8099 - acc: 0.3172\n",
      "Epoch 146/200\n",
      "5435/5435 [==============================] - 2s 335us/step - loss: 1.8088 - acc: 0.3154\n",
      "Epoch 147/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.8101 - acc: 0.3192\n",
      "Epoch 148/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.8154 - acc: 0.3155\n",
      "Epoch 149/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.8019 - acc: 0.3249\n",
      "Epoch 150/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 1.8081 - acc: 0.3247\n",
      "Epoch 151/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.8110 - acc: 0.3201\n",
      "Epoch 152/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.7919 - acc: 0.3227\n",
      "Epoch 153/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.7987 - acc: 0.3150\n",
      "Epoch 154/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.8017 - acc: 0.3183\n",
      "Epoch 155/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.7979 - acc: 0.3244\n",
      "Epoch 156/200\n",
      "5435/5435 [==============================] - 2s 341us/step - loss: 1.7924 - acc: 0.3257\n",
      "Epoch 157/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.7966 - acc: 0.3231\n",
      "Epoch 158/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 1.7926 - acc: 0.3275\n",
      "Epoch 159/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.7969 - acc: 0.3196\n",
      "Epoch 160/200\n",
      "5435/5435 [==============================] - 2s 359us/step - loss: 1.7946 - acc: 0.3192\n",
      "Epoch 161/200\n",
      "5435/5435 [==============================] - 2s 348us/step - loss: 1.7892 - acc: 0.3214\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.7904 - acc: 0.3255\n",
      "Epoch 163/200\n",
      "5435/5435 [==============================] - 2s 346us/step - loss: 1.7923 - acc: 0.3207\n",
      "Epoch 164/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.7868 - acc: 0.3255\n",
      "Epoch 165/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.7810 - acc: 0.3281\n",
      "Epoch 166/200\n",
      "5435/5435 [==============================] - 2s 334us/step - loss: 1.7816 - acc: 0.3255\n",
      "Epoch 167/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.7902 - acc: 0.3207\n",
      "Epoch 168/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.7799 - acc: 0.3257\n",
      "Epoch 169/200\n",
      "5435/5435 [==============================] - 2s 335us/step - loss: 1.7858 - acc: 0.3135\n",
      "Epoch 170/200\n",
      "5435/5435 [==============================] - 2s 335us/step - loss: 1.7860 - acc: 0.3273\n",
      "Epoch 171/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.7864 - acc: 0.3139\n",
      "Epoch 172/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.7825 - acc: 0.3240\n",
      "Epoch 173/200\n",
      "5435/5435 [==============================] - 2s 335us/step - loss: 1.7786 - acc: 0.3317\n",
      "Epoch 174/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.7864 - acc: 0.3222\n",
      "Epoch 175/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.7800 - acc: 0.3308\n",
      "Epoch 176/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.7758 - acc: 0.3246\n",
      "Epoch 177/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.7816 - acc: 0.3216\n",
      "Epoch 178/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.7861 - acc: 0.3233\n",
      "Epoch 179/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.7846 - acc: 0.3189\n",
      "Epoch 180/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 1.7758 - acc: 0.3316\n",
      "Epoch 181/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.7788 - acc: 0.3260\n",
      "Epoch 182/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.7708 - acc: 0.3282\n",
      "Epoch 183/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.7739 - acc: 0.3338\n",
      "Epoch 184/200\n",
      "5435/5435 [==============================] - 2s 332us/step - loss: 1.7697 - acc: 0.3253\n",
      "Epoch 185/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.7715 - acc: 0.3297\n",
      "Epoch 186/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.7754 - acc: 0.3260\n",
      "Epoch 187/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.7714 - acc: 0.3260\n",
      "Epoch 188/200\n",
      "5435/5435 [==============================] - 2s 441us/step - loss: 1.7651 - acc: 0.3290\n",
      "Epoch 189/200\n",
      "5435/5435 [==============================] - 2s 403us/step - loss: 1.7657 - acc: 0.3246\n",
      "Epoch 190/200\n",
      "5435/5435 [==============================] - 2s 341us/step - loss: 1.7699 - acc: 0.3332\n",
      "Epoch 191/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.7600 - acc: 0.3290\n",
      "Epoch 192/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.7689 - acc: 0.3295\n",
      "Epoch 193/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.7699 - acc: 0.3284\n",
      "Epoch 194/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 1.7686 - acc: 0.3264\n",
      "Epoch 195/200\n",
      "5435/5435 [==============================] - 2s 339us/step - loss: 1.7575 - acc: 0.3336\n",
      "Epoch 196/200\n",
      "5435/5435 [==============================] - 2s 336us/step - loss: 1.7620 - acc: 0.3384\n",
      "Epoch 197/200\n",
      "5435/5435 [==============================] - 1s 161us/step - loss: 1.7695 - acc: 0.3312\n",
      "Epoch 198/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 1.7683 - acc: 0.3325\n",
      "Epoch 199/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 1.7673 - acc: 0.3341\n",
      "Epoch 200/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 1.7608 - acc: 0.3284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', <keras.wrappers.scikit_learn.KerasClassifier object at 0x1c4890d160>)])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building a pipeline and fitting it\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=200, batch_size=1000, verbose=1, )))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "pipeline.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross-validating pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Row: 700\n",
      "Handling Row: 800\n",
      "Handling Row: 1600\n",
      "Handling Row: 1700\n",
      "Handling Row: 1900\n",
      "Handling Row: 2100\n",
      "Handling Row: 2400\n",
      "Handling Row: 2500\n",
      "Handling Row: 3000\n",
      "Handling Row: 3100\n",
      "Handling Row: 3300\n",
      "Handling Row: 3500\n",
      "Handling Row: 3600\n",
      "Handling Row: 4300\n",
      "Handling Row: 4600\n",
      "Handling Row: 4700\n",
      "Handling Row: 5000\n",
      "Handling Row: 5300\n",
      "Handling Row: 5500\n",
      "Handling Row: 5700\n",
      "Handling Row: 6000\n",
      "Handling Row: 6200\n",
      "Handling Row: 6400\n",
      "Handling Row: 6700\n",
      "Handling Row: 7300\n",
      "Handling Row: 8400\n",
      "Handling Row: 8600\n",
      "Handling Row: 8700\n"
     ]
    }
   ],
   "source": [
    "## creating a test dataframe\n",
    "\n",
    "test = pd.read_csv('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/test.csv')\n",
    "\n",
    "def feature_extractor_t(row):\n",
    "    # function to load files and extract features\n",
    "    file_name = os.path.join(os.path.abspath('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/'),\n",
    "                            'Test', str(row.ID) + '.wav')\n",
    "    if (row.ID%100 ==0):\n",
    "        print (\"Handling Row:\", row.ID)\n",
    "\n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        \n",
    "        \n",
    "        #calculating ZCR feature\n",
    "        from librosa.feature import zero_crossing_rate\n",
    "        from librosa.core import load \n",
    "\n",
    "        ##padding with zeros to make a constant length of feature across all audios\n",
    "        zcr_padded = np.zeros((210,))\n",
    "        zcr = zero_crossing_rate(X, frame_length=2048, hop_length=441)\n",
    "        zcr_padded[:zcr[0].shape[0]] = zcr[0]\n",
    "        zcr_padded\n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", e)\n",
    "        return None\n",
    "\n",
    "    features = zcr_padded\n",
    "    \n",
    "\n",
    "    return features.tolist()\n",
    "\n",
    "new_test = test.apply(feature_extractor_t, axis=1)\n",
    "new_test.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3297/3297 [==============================] - 1s 158us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_t = np.array(new_test.tolist())\n",
    "\n",
    "lb.inverse_transform\n",
    "\n",
    "test['Class']=lb.inverse_transform(pipeline.predict(X_t))\n",
    "\n",
    "test.to_csv('predictions_3.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PSD feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extractor(row):\n",
    "    \n",
    "    # defining path to train files\n",
    "    file_name = os.path.join(os.path.abspath('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/'),\n",
    "                            'Train', str(row.ID) + '.wav')\n",
    "    if (row.ID%100 ==0):\n",
    "        print (\"Handling Row:\", row.ID)\n",
    "    \n",
    "    #enclosing all this in try-catch to handle exceptions and missing files\n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "\n",
    "        \n",
    "        \n",
    "        #calculating PSD feature\n",
    "        from scipy import signal\n",
    "        import matplotlib.pyplot as plt\n",
    "        from madmom.audio.signal import Signal\n",
    "\n",
    "        ##calculating PSD using Welch's method\n",
    "        s = Signal(file_name, num_channels=1)\n",
    "        f, P = signal.welch(s, 44100, nperseg=1024)\n",
    "        \n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None, None\n",
    "\n",
    "    features = P\n",
    "    label = row.Class\n",
    "\n",
    "    return [features, label]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Row: 0\n",
      "Handling Row: 0\n",
      "Handling Row: 100\n",
      "Handling Row: 200\n",
      "Handling Row: 300\n",
      "Handling Row: 400\n",
      "Handling Row: 500\n",
      "Handling Row: 600\n",
      "Handling Row: 900\n",
      "Handling Row: 1000\n",
      "Handling Row: 1100\n",
      "Handling Row: 1200\n",
      "Handling Row: 1300\n",
      "Handling Row: 1400\n",
      "Handling Row: 1500\n",
      "Handling Row: 1800\n",
      "Handling Row: 2000\n",
      "Handling Row: 2200\n",
      "Handling Row: 2300\n",
      "Handling Row: 2600\n",
      "Handling Row: 2700\n",
      "Handling Row: 2800\n",
      "Handling Row: 2900\n",
      "Handling Row: 3200\n",
      "Handling Row: 3400\n",
      "Handling Row: 3700\n",
      "Handling Row: 3800\n",
      "Handling Row: 3900\n",
      "Handling Row: 4000\n",
      "Handling Row: 4100\n",
      "Handling Row: 4200\n",
      "Handling Row: 4400\n",
      "Handling Row: 4500\n",
      "Handling Row: 4800\n",
      "Handling Row: 4900\n",
      "Handling Row: 5100\n",
      "Handling Row: 5200\n",
      "Handling Row: 5400\n",
      "Handling Row: 5600\n",
      "Handling Row: 5800\n",
      "Handling Row: 5900\n",
      "Handling Row: 6100\n",
      "Handling Row: 6300\n",
      "Handling Row: 6500\n",
      "Handling Row: 6600\n",
      "Handling Row: 6800\n",
      "Handling Row: 6900\n",
      "Handling Row: 7000\n",
      "Handling Row: 7100\n",
      "Handling Row: 7200\n",
      "Handling Row: 7400\n",
      "Handling Row: 7500\n",
      "Handling Row: 7600\n",
      "Handling Row: 7700\n",
      "Handling Row: 7800\n",
      "Handling Row: 7900\n",
      "Handling Row: 8000\n",
      "Handling Row: 8100\n",
      "Handling Row: 8200\n",
      "Handling Row: 8300\n",
      "Handling Row: 8500\n"
     ]
    }
   ],
   "source": [
    "#applying feature_estractor on each row in train \n",
    "new_train = train.apply(feature_extractor, axis=1)\n",
    "new_train.columns = ['features', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5435, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping rows for missing files\n",
    "new_train.dropna(inplace=True)\n",
    "new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.86222583e+03,   2.71326094e+04,   5.44416797e+04, ...,\n",
       "          4.85184046e-06,   5.37353344e-06,   2.61063838e-06],\n",
       "       [  2.51026025e+03,   8.72691602e+03,   2.61261660e+04, ...,\n",
       "          4.33236528e-06,   4.08082587e-06,   1.83681414e-06],\n",
       "       [  2.43888206e+01,   7.72827759e+01,   9.21221313e+01, ...,\n",
       "          2.31134982e-05,   2.47511634e-05,   1.34520560e-05],\n",
       "       ..., \n",
       "       [  4.73914642e+01,   2.42590179e+02,   1.93339615e+02, ...,\n",
       "          1.02952309e-03,   9.46316228e-04,   4.66408586e-04],\n",
       "       [  3.02349341e+03,   1.26310723e+04,   5.02322705e+03, ...,\n",
       "          7.78507456e-06,   1.19406022e-05,   1.07108226e-05],\n",
       "       [  7.73747754e+03,   5.44574219e+04,   5.64075703e+04, ...,\n",
       "          7.99128175e-05,   8.25619281e-05,   3.87824657e-05]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing data for feeding into the model\n",
    "X = np.array(new_train.features.tolist())\n",
    "y = np.array(new_train.label.tolist())\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y = np_utils.to_categorical(lb.fit_transform(y))\n",
    "\n",
    "num_labels = y.shape[1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building the model architecture\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1024, input_shape=(513,), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1024, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(512, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd= SGD(momentum=0.2, lr=0.03)\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "    return model\n",
    "          \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5435/5435 [==============================] - 3s 492us/step - loss: 2.4416 - acc: 0.1071\n",
      "Epoch 2/200\n",
      "5435/5435 [==============================] - 2s 408us/step - loss: 2.3382 - acc: 0.1178\n",
      "Epoch 3/200\n",
      "5435/5435 [==============================] - 2s 389us/step - loss: 2.2937 - acc: 0.1369\n",
      "Epoch 4/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 2.2733 - acc: 0.1468\n",
      "Epoch 5/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 2.2644 - acc: 0.1632\n",
      "Epoch 6/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 2.2675 - acc: 0.1588\n",
      "Epoch 7/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 2.2452 - acc: 0.1623\n",
      "Epoch 8/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 2.2168 - acc: 0.1777\n",
      "Epoch 9/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 2.2190 - acc: 0.1737\n",
      "Epoch 10/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 2.2043 - acc: 0.1805\n",
      "Epoch 11/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 2.1913 - acc: 0.1890\n",
      "Epoch 12/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 2.1728 - acc: 0.1983\n",
      "Epoch 13/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 2.1728 - acc: 0.1960\n",
      "Epoch 14/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 2.1603 - acc: 0.2132\n",
      "Epoch 15/200\n",
      "5435/5435 [==============================] - 2s 369us/step - loss: 2.1498 - acc: 0.2127\n",
      "Epoch 16/200\n",
      "5435/5435 [==============================] - 2s 379us/step - loss: 2.1406 - acc: 0.2167\n",
      "Epoch 17/200\n",
      "5435/5435 [==============================] - 2s 371us/step - loss: 2.1366 - acc: 0.2294\n",
      "Epoch 18/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 2.1215 - acc: 0.2237\n",
      "Epoch 19/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 2.1036 - acc: 0.2243\n",
      "Epoch 20/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 2.1092 - acc: 0.2250\n",
      "Epoch 21/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 2.0974 - acc: 0.2425\n",
      "Epoch 22/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 2.0897 - acc: 0.2366\n",
      "Epoch 23/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 2.0770 - acc: 0.2405\n",
      "Epoch 24/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 2.0758 - acc: 0.2508\n",
      "Epoch 25/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 2.0670 - acc: 0.2535\n",
      "Epoch 26/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 2.0568 - acc: 0.2524\n",
      "Epoch 27/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 2.0447 - acc: 0.2554\n",
      "Epoch 28/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 2.0463 - acc: 0.2668\n",
      "Epoch 29/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 2.0234 - acc: 0.2699\n",
      "Epoch 30/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 2.0142 - acc: 0.2686\n",
      "Epoch 31/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 2.0047 - acc: 0.2782\n",
      "Epoch 32/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 2.0152 - acc: 0.2751\n",
      "Epoch 33/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 2.0056 - acc: 0.2670\n",
      "Epoch 34/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.9925 - acc: 0.2789\n",
      "Epoch 35/200\n",
      "5435/5435 [==============================] - 2s 379us/step - loss: 1.9897 - acc: 0.2822\n",
      "Epoch 36/200\n",
      "5435/5435 [==============================] - 2s 410us/step - loss: 1.9787 - acc: 0.2830\n",
      "Epoch 37/200\n",
      "5435/5435 [==============================] - 2s 387us/step - loss: 1.9888 - acc: 0.2767\n",
      "Epoch 38/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.9693 - acc: 0.2929\n",
      "Epoch 39/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.9660 - acc: 0.2874\n",
      "Epoch 40/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.9549 - acc: 0.2981\n",
      "Epoch 41/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.9595 - acc: 0.2959\n",
      "Epoch 42/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.9448 - acc: 0.2994\n",
      "Epoch 43/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.9463 - acc: 0.3027\n",
      "Epoch 44/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.9275 - acc: 0.3122\n",
      "Epoch 45/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.9253 - acc: 0.3087\n",
      "Epoch 46/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.9110 - acc: 0.3135\n",
      "Epoch 47/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.9125 - acc: 0.3126\n",
      "Epoch 48/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.9055 - acc: 0.3179\n",
      "Epoch 49/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.8956 - acc: 0.3165\n",
      "Epoch 50/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.8943 - acc: 0.3141\n",
      "Epoch 51/200\n",
      "5435/5435 [==============================] - 2s 376us/step - loss: 1.8943 - acc: 0.3154\n",
      "Epoch 52/200\n",
      "5435/5435 [==============================] - 2s 379us/step - loss: 1.8809 - acc: 0.3233\n",
      "Epoch 53/200\n",
      "5435/5435 [==============================] - 2s 369us/step - loss: 1.8832 - acc: 0.3299\n",
      "Epoch 54/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.8709 - acc: 0.3229\n",
      "Epoch 55/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.8631 - acc: 0.3360\n",
      "Epoch 56/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.8615 - acc: 0.3310\n",
      "Epoch 57/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.8533 - acc: 0.3330\n",
      "Epoch 58/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.8510 - acc: 0.3369\n",
      "Epoch 59/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.8498 - acc: 0.3433\n",
      "Epoch 60/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.8353 - acc: 0.3474\n",
      "Epoch 61/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.8418 - acc: 0.3363\n",
      "Epoch 62/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.8244 - acc: 0.3498\n",
      "Epoch 63/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.8286 - acc: 0.3411\n",
      "Epoch 64/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.8178 - acc: 0.3430\n",
      "Epoch 65/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.8108 - acc: 0.3527\n",
      "Epoch 66/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.8183 - acc: 0.3584\n",
      "Epoch 67/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.8203 - acc: 0.3522\n",
      "Epoch 68/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.8132 - acc: 0.3577\n",
      "Epoch 69/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.8137 - acc: 0.3562\n",
      "Epoch 70/200\n",
      "5435/5435 [==============================] - 2s 399us/step - loss: 1.7918 - acc: 0.3689\n",
      "Epoch 71/200\n",
      "5435/5435 [==============================] - 3s 476us/step - loss: 1.7966 - acc: 0.3492\n",
      "Epoch 72/200\n",
      "5435/5435 [==============================] - 2s 435us/step - loss: 1.8030 - acc: 0.3562\n",
      "Epoch 73/200\n",
      "5435/5435 [==============================] - 2s 369us/step - loss: 1.7827 - acc: 0.3621\n",
      "Epoch 74/200\n",
      "5435/5435 [==============================] - 2s 369us/step - loss: 1.7844 - acc: 0.3593\n",
      "Epoch 75/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.7747 - acc: 0.3733\n",
      "Epoch 76/200\n",
      "5435/5435 [==============================] - 2s 382us/step - loss: 1.7736 - acc: 0.3702\n",
      "Epoch 77/200\n",
      "5435/5435 [==============================] - 2s 373us/step - loss: 1.7746 - acc: 0.3706\n",
      "Epoch 78/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.7545 - acc: 0.3836\n",
      "Epoch 79/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.7503 - acc: 0.3735\n",
      "Epoch 80/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.7585 - acc: 0.3728\n",
      "Epoch 81/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.7647 - acc: 0.3718\n",
      "Epoch 82/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.7525 - acc: 0.3809\n",
      "Epoch 83/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.7417 - acc: 0.3890\n",
      "Epoch 84/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.7565 - acc: 0.3783\n",
      "Epoch 85/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.7326 - acc: 0.3856\n",
      "Epoch 86/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.7365 - acc: 0.3823\n",
      "Epoch 87/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.7292 - acc: 0.3875\n",
      "Epoch 88/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.7364 - acc: 0.3877\n",
      "Epoch 89/200\n",
      "5435/5435 [==============================] - 2s 370us/step - loss: 1.7173 - acc: 0.3890\n",
      "Epoch 90/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.7250 - acc: 0.3914\n",
      "Epoch 91/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.7216 - acc: 0.3974\n",
      "Epoch 92/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.7160 - acc: 0.3888\n",
      "Epoch 93/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.7213 - acc: 0.3993\n",
      "Epoch 94/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.7129 - acc: 0.3912\n",
      "Epoch 95/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.7017 - acc: 0.3932\n",
      "Epoch 96/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.7032 - acc: 0.4015\n",
      "Epoch 97/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.6841 - acc: 0.4017\n",
      "Epoch 98/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6841 - acc: 0.3985\n",
      "Epoch 99/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.7011 - acc: 0.3978\n",
      "Epoch 100/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6896 - acc: 0.4081\n",
      "Epoch 101/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.6897 - acc: 0.4085\n",
      "Epoch 102/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.6802 - acc: 0.4151\n",
      "Epoch 103/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6785 - acc: 0.4079\n",
      "Epoch 104/200\n",
      "5435/5435 [==============================] - 2s 369us/step - loss: 1.6898 - acc: 0.4079\n",
      "Epoch 105/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6680 - acc: 0.4079\n",
      "Epoch 106/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.6787 - acc: 0.4094\n",
      "Epoch 107/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.6647 - acc: 0.4110\n",
      "Epoch 108/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.6508 - acc: 0.4155\n",
      "Epoch 109/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.6591 - acc: 0.4294\n",
      "Epoch 110/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.6494 - acc: 0.4259\n",
      "Epoch 111/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.6642 - acc: 0.4099\n",
      "Epoch 112/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.6604 - acc: 0.4149\n",
      "Epoch 113/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6396 - acc: 0.4278\n",
      "Epoch 114/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.6439 - acc: 0.4171\n",
      "Epoch 115/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6440 - acc: 0.4202\n",
      "Epoch 116/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.6441 - acc: 0.4166\n",
      "Epoch 117/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6316 - acc: 0.4296\n",
      "Epoch 118/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6412 - acc: 0.4241\n",
      "Epoch 119/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.6303 - acc: 0.4302\n",
      "Epoch 120/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6259 - acc: 0.4311\n",
      "Epoch 121/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.6202 - acc: 0.4362\n",
      "Epoch 122/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.6297 - acc: 0.4298\n",
      "Epoch 123/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.6127 - acc: 0.4403\n",
      "Epoch 124/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6077 - acc: 0.4333\n",
      "Epoch 125/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6086 - acc: 0.4386\n",
      "Epoch 126/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.6063 - acc: 0.4440\n",
      "Epoch 127/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.6159 - acc: 0.4385\n",
      "Epoch 128/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.5997 - acc: 0.4408\n",
      "Epoch 129/200\n",
      "5435/5435 [==============================] - 2s 384us/step - loss: 1.5985 - acc: 0.4447\n",
      "Epoch 130/200\n",
      "5435/5435 [==============================] - 3s 470us/step - loss: 1.6049 - acc: 0.4462\n",
      "Epoch 131/200\n",
      "5435/5435 [==============================] - 2s 426us/step - loss: 1.5960 - acc: 0.4438\n",
      "Epoch 132/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5918 - acc: 0.4486\n",
      "Epoch 133/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.5831 - acc: 0.4504\n",
      "Epoch 134/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.5847 - acc: 0.4508\n",
      "Epoch 135/200\n",
      "5435/5435 [==============================] - 2s 376us/step - loss: 1.5813 - acc: 0.4546\n",
      "Epoch 136/200\n",
      "5435/5435 [==============================] - 2s 371us/step - loss: 1.5744 - acc: 0.4532\n",
      "Epoch 137/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5743 - acc: 0.4467\n",
      "Epoch 138/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.5716 - acc: 0.4541\n",
      "Epoch 139/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5694 - acc: 0.4543\n",
      "Epoch 140/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5745 - acc: 0.4431\n",
      "Epoch 141/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5734 - acc: 0.4491\n",
      "Epoch 142/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5696 - acc: 0.4583\n",
      "Epoch 143/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.5610 - acc: 0.4649\n",
      "Epoch 144/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.5605 - acc: 0.4477\n",
      "Epoch 145/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.5621 - acc: 0.4618\n",
      "Epoch 146/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.5525 - acc: 0.4668\n",
      "Epoch 147/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.5646 - acc: 0.4618\n",
      "Epoch 148/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.5508 - acc: 0.4602\n",
      "Epoch 149/200\n",
      "5435/5435 [==============================] - 2s 369us/step - loss: 1.5361 - acc: 0.4725\n",
      "Epoch 150/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.5490 - acc: 0.4644\n",
      "Epoch 151/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.5527 - acc: 0.4655\n",
      "Epoch 152/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.5474 - acc: 0.4659\n",
      "Epoch 153/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.5242 - acc: 0.4692\n",
      "Epoch 154/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.5426 - acc: 0.4644\n",
      "Epoch 155/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5520 - acc: 0.4627\n",
      "Epoch 156/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5379 - acc: 0.4613\n",
      "Epoch 157/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5235 - acc: 0.4771\n",
      "Epoch 158/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.5362 - acc: 0.4673\n",
      "Epoch 159/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5227 - acc: 0.4727\n",
      "Epoch 160/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5265 - acc: 0.4615\n",
      "Epoch 161/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5258 - acc: 0.4670\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.5335 - acc: 0.4686\n",
      "Epoch 163/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.5211 - acc: 0.4804\n",
      "Epoch 164/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.5163 - acc: 0.4771\n",
      "Epoch 165/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.4995 - acc: 0.4786\n",
      "Epoch 166/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.5123 - acc: 0.4775\n",
      "Epoch 167/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.5075 - acc: 0.4743\n",
      "Epoch 168/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5100 - acc: 0.4732\n",
      "Epoch 169/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.4987 - acc: 0.4845\n",
      "Epoch 170/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.5063 - acc: 0.4791\n",
      "Epoch 171/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.5083 - acc: 0.4758\n",
      "Epoch 172/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.4997 - acc: 0.4789\n",
      "Epoch 173/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.5032 - acc: 0.4771\n",
      "Epoch 174/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.5038 - acc: 0.4810\n",
      "Epoch 175/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.4878 - acc: 0.4797\n",
      "Epoch 176/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.4827 - acc: 0.4852\n",
      "Epoch 177/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.4896 - acc: 0.4804\n",
      "Epoch 178/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.4861 - acc: 0.4830\n",
      "Epoch 179/200\n",
      "5435/5435 [==============================] - 2s 369us/step - loss: 1.4802 - acc: 0.4857\n",
      "Epoch 180/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.4816 - acc: 0.4863\n",
      "Epoch 181/200\n",
      "5435/5435 [==============================] - 2s 365us/step - loss: 1.4784 - acc: 0.4913\n",
      "Epoch 182/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.4752 - acc: 0.4984\n",
      "Epoch 183/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.4764 - acc: 0.4828\n",
      "Epoch 184/200\n",
      "5435/5435 [==============================] - 2s 373us/step - loss: 1.4735 - acc: 0.4889\n",
      "Epoch 185/200\n",
      "5435/5435 [==============================] - 2s 383us/step - loss: 1.4583 - acc: 0.4973\n",
      "Epoch 186/200\n",
      "5435/5435 [==============================] - 2s 376us/step - loss: 1.4688 - acc: 0.4889\n",
      "Epoch 187/200\n",
      "5435/5435 [==============================] - 2s 380us/step - loss: 1.4580 - acc: 0.4850\n",
      "Epoch 188/200\n",
      "5435/5435 [==============================] - 2s 383us/step - loss: 1.4710 - acc: 0.4909\n",
      "Epoch 189/200\n",
      "5435/5435 [==============================] - 2s 369us/step - loss: 1.4634 - acc: 0.4927\n",
      "Epoch 190/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.4638 - acc: 0.4964\n",
      "Epoch 191/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.4640 - acc: 0.4949\n",
      "Epoch 192/200\n",
      "5435/5435 [==============================] - 2s 366us/step - loss: 1.4613 - acc: 0.4828\n",
      "Epoch 193/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 1.4556 - acc: 0.4914\n",
      "Epoch 194/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.4575 - acc: 0.4964\n",
      "Epoch 195/200\n",
      "5435/5435 [==============================] - 2s 377us/step - loss: 1.4432 - acc: 0.4986\n",
      "Epoch 196/200\n",
      "5435/5435 [==============================] - 2s 370us/step - loss: 1.4393 - acc: 0.5027\n",
      "Epoch 197/200\n",
      "5435/5435 [==============================] - 2s 368us/step - loss: 1.4477 - acc: 0.4946\n",
      "Epoch 198/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.4389 - acc: 0.4937\n",
      "Epoch 199/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.4547 - acc: 0.5006\n",
      "Epoch 200/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.4494 - acc: 0.4942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', <keras.wrappers.scikit_learn.KerasClassifier object at 0x1c221e2d68>)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building a pipeline and fitting it\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=200, batch_size=1000, verbose=1, )))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "pipeline.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross-validating pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Row: 700\n",
      "Handling Row: 800\n",
      "Handling Row: 1600\n",
      "Handling Row: 1700\n",
      "Handling Row: 1900\n",
      "Handling Row: 2100\n",
      "Handling Row: 2400\n",
      "Handling Row: 2500\n",
      "Handling Row: 3000\n",
      "Handling Row: 3100\n",
      "Handling Row: 3300\n",
      "Handling Row: 3500\n",
      "Handling Row: 3600\n",
      "Handling Row: 4300\n",
      "Handling Row: 4600\n",
      "Handling Row: 4700\n",
      "Handling Row: 5000\n",
      "Handling Row: 5300\n",
      "Handling Row: 5500\n",
      "Handling Row: 5700\n",
      "Handling Row: 6000\n",
      "Handling Row: 6200\n",
      "Handling Row: 6400\n",
      "Handling Row: 6700\n",
      "Handling Row: 7300\n",
      "Handling Row: 8400\n",
      "Handling Row: 8600\n",
      "Handling Row: 8700\n"
     ]
    }
   ],
   "source": [
    "## creating a test dataframe\n",
    "\n",
    "test = pd.read_csv('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/test.csv')\n",
    "\n",
    "def feature_extractor_t(row):\n",
    "    # function to load files and extract features\n",
    "    file_name = os.path.join(os.path.abspath('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/'),\n",
    "                            'Test', str(row.ID) + '.wav')\n",
    "    if (row.ID%100 ==0):\n",
    "        print (\"Handling Row:\", row.ID)\n",
    "\n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        \n",
    "        \n",
    "        #calculating PSD feature\n",
    "        from scipy import signal\n",
    "        import matplotlib.pyplot as plt\n",
    "        from madmom.audio.signal import Signal\n",
    "\n",
    "        ##calculating PSD using Welch's method\n",
    "        s = Signal(file_name, num_channels=1)\n",
    "        f, P = signal.welch(s, 44100, nperseg=1024)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "\n",
    "    features = P\n",
    "    \n",
    "\n",
    "    return features.tolist()\n",
    "\n",
    "new_test = test.apply(feature_extractor_t, axis=1)\n",
    "new_test.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_t = np.array(new_test.tolist())\n",
    "\n",
    "lb.inverse_transform\n",
    "\n",
    "test['Class']=lb.inverse_transform(pipeline.predict(X_t))\n",
    "\n",
    "test.to_csv('predictions_4.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MFCC feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extractor(row):\n",
    "    \n",
    "    # defining path to train files\n",
    "    file_name = os.path.join(os.path.abspath('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/'),\n",
    "                            'Train', str(row.ID) + '.wav')\n",
    "    \n",
    "    #enclosing all this in try-catch to handle exceptions and missing files\n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "\n",
    "        \n",
    "        \n",
    "        #calculating MFCC feature\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=39).T,axis=0) \n",
    "       \n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None, None\n",
    "\n",
    "    features = mfccs\n",
    "    label = row.Class\n",
    "\n",
    "    return [features, label]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying feature_estractor on each row in train \n",
    "new_train = train.apply(feature_extractor, axis=1)\n",
    "new_train.columns = ['features', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5435, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping rows for missing files\n",
    "new_train.dropna(inplace=True)\n",
    "new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -82.11494596,  139.47317581,  -42.41008515,   24.79492588,\n",
       "        -11.59744647,   23.47668213,  -12.16276146,   25.87477093,\n",
       "         -9.37786981,   21.19287823,   -7.34720007,   14.23773422,\n",
       "         -8.65836673,    7.72666064,  -10.11617199,    3.2386625 ,\n",
       "        -11.33468734,    2.78280142,   -7.03527193,    3.89992113,\n",
       "         -2.32080626,    2.00126504,   -2.78118037,    4.11963711,\n",
       "         -1.61454706,    4.31963577,   -1.02849855,   -1.24585996,\n",
       "         -3.106575  ,    0.31221309,   -1.78428594,    0.43504187,\n",
       "         -1.78772245,   -0.76478368,   -1.23643351,   -0.27920185,\n",
       "          0.66139824,   -0.50357525,   -2.60634411])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing data for feeding into the model\n",
    "X = np.array(new_train.features.tolist())\n",
    "y = np.array(new_train.label.tolist())\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y = np_utils.to_categorical(lb.fit_transform(y))\n",
    "\n",
    "num_labels = y.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building the model architecture\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1024, input_shape=(39,), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1024, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(512, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd= SGD(momentum=0.2, lr=0.03)\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "    return model\n",
    "          \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5435/5435 [==============================] - 3s 494us/step - loss: 2.3956 - acc: 0.0992\n",
      "Epoch 2/200\n",
      "5435/5435 [==============================] - 2s 452us/step - loss: 2.3010 - acc: 0.1321\n",
      "Epoch 3/200\n",
      "5435/5435 [==============================] - 3s 522us/step - loss: 2.2491 - acc: 0.1582\n",
      "Epoch 4/200\n",
      "5435/5435 [==============================] - 3s 485us/step - loss: 2.2069 - acc: 0.1862\n",
      "Epoch 5/200\n",
      "5435/5435 [==============================] - 2s 396us/step - loss: 2.1778 - acc: 0.2160\n",
      "Epoch 6/200\n",
      "5435/5435 [==============================] - 3s 468us/step - loss: 2.1368 - acc: 0.2405\n",
      "Epoch 7/200\n",
      "5435/5435 [==============================] - 3s 487us/step - loss: 2.0912 - acc: 0.2600\n",
      "Epoch 8/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 2.0616 - acc: 0.2826\n",
      "Epoch 9/200\n",
      "5435/5435 [==============================] - 2s 379us/step - loss: 2.0233 - acc: 0.2957\n",
      "Epoch 10/200\n",
      "5435/5435 [==============================] - 2s 345us/step - loss: 1.9861 - acc: 0.3174\n",
      "Epoch 11/200\n",
      "5435/5435 [==============================] - 2s 343us/step - loss: 1.9449 - acc: 0.3351\n",
      "Epoch 12/200\n",
      "5435/5435 [==============================] - 2s 341us/step - loss: 1.9032 - acc: 0.3472\n",
      "Epoch 13/200\n",
      "5435/5435 [==============================] - 2s 346us/step - loss: 1.8637 - acc: 0.3663\n",
      "Epoch 14/200\n",
      "5435/5435 [==============================] - 2s 348us/step - loss: 1.8342 - acc: 0.3737\n",
      "Epoch 15/200\n",
      "5435/5435 [==============================] - 2s 347us/step - loss: 1.8094 - acc: 0.3698\n",
      "Epoch 16/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.7641 - acc: 0.4031\n",
      "Epoch 17/200\n",
      "5435/5435 [==============================] - 2s 353us/step - loss: 1.7408 - acc: 0.4063\n",
      "Epoch 18/200\n",
      "5435/5435 [==============================] - 2s 354us/step - loss: 1.6999 - acc: 0.4096\n",
      "Epoch 19/200\n",
      "5435/5435 [==============================] - 2s 349us/step - loss: 1.6771 - acc: 0.4232\n",
      "Epoch 20/200\n",
      "5435/5435 [==============================] - 2s 355us/step - loss: 1.6475 - acc: 0.4307\n",
      "Epoch 21/200\n",
      "5435/5435 [==============================] - 2s 356us/step - loss: 1.6338 - acc: 0.4355\n",
      "Epoch 22/200\n",
      "5435/5435 [==============================] - 2s 345us/step - loss: 1.6027 - acc: 0.4495\n",
      "Epoch 23/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.5703 - acc: 0.4589\n",
      "Epoch 24/200\n",
      "5435/5435 [==============================] - 2s 398us/step - loss: 1.5622 - acc: 0.4559\n",
      "Epoch 25/200\n",
      "5435/5435 [==============================] - 2s 345us/step - loss: 1.5397 - acc: 0.4699\n",
      "Epoch 26/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.5064 - acc: 0.4848\n",
      "Epoch 27/200\n",
      "5435/5435 [==============================] - 2s 342us/step - loss: 1.4880 - acc: 0.4905\n",
      "Epoch 28/200\n",
      "5435/5435 [==============================] - 2s 353us/step - loss: 1.4647 - acc: 0.4966\n",
      "Epoch 29/200\n",
      "5435/5435 [==============================] - 2s 357us/step - loss: 1.4532 - acc: 0.4970\n",
      "Epoch 30/200\n",
      "5435/5435 [==============================] - 2s 372us/step - loss: 1.4495 - acc: 0.5016\n",
      "Epoch 31/200\n",
      "5435/5435 [==============================] - 2s 359us/step - loss: 1.4124 - acc: 0.5205\n",
      "Epoch 32/200\n",
      "5435/5435 [==============================] - 2s 358us/step - loss: 1.3999 - acc: 0.5167\n",
      "Epoch 33/200\n",
      "5435/5435 [==============================] - 2s 361us/step - loss: 1.3834 - acc: 0.5279\n",
      "Epoch 34/200\n",
      "5435/5435 [==============================] - 2s 363us/step - loss: 1.3630 - acc: 0.5306\n",
      "Epoch 35/200\n",
      "5435/5435 [==============================] - 2s 362us/step - loss: 1.3374 - acc: 0.5444\n",
      "Epoch 36/200\n",
      "5435/5435 [==============================] - 2s 360us/step - loss: 1.3217 - acc: 0.5505\n",
      "Epoch 37/200\n",
      "5435/5435 [==============================] - 2s 367us/step - loss: 1.3146 - acc: 0.5492\n",
      "Epoch 38/200\n",
      "5435/5435 [==============================] - 2s 355us/step - loss: 1.3169 - acc: 0.5544\n",
      "Epoch 39/200\n",
      "5435/5435 [==============================] - 2s 356us/step - loss: 1.2846 - acc: 0.5682\n",
      "Epoch 40/200\n",
      "5435/5435 [==============================] - 2s 399us/step - loss: 1.2712 - acc: 0.5691\n",
      "Epoch 41/200\n",
      "5435/5435 [==============================] - 2s 454us/step - loss: 1.2666 - acc: 0.5661\n",
      "Epoch 42/200\n",
      "5435/5435 [==============================] - 2s 385us/step - loss: 1.2463 - acc: 0.5844\n",
      "Epoch 43/200\n",
      "5435/5435 [==============================] - 2s 376us/step - loss: 1.2338 - acc: 0.5796\n",
      "Epoch 44/200\n",
      "5435/5435 [==============================] - 2s 372us/step - loss: 1.2335 - acc: 0.5774\n",
      "Epoch 45/200\n",
      "5435/5435 [==============================] - 2s 374us/step - loss: 1.2252 - acc: 0.5810\n",
      "Epoch 46/200\n",
      "5435/5435 [==============================] - 2s 449us/step - loss: 1.1975 - acc: 0.6020\n",
      "Epoch 47/200\n",
      "5435/5435 [==============================] - 2s 402us/step - loss: 1.1851 - acc: 0.5936\n",
      "Epoch 48/200\n",
      "5435/5435 [==============================] - 2s 344us/step - loss: 1.1834 - acc: 0.5902\n",
      "Epoch 49/200\n",
      "5435/5435 [==============================] - 2s 330us/step - loss: 1.1744 - acc: 0.6020\n",
      "Epoch 50/200\n",
      "5435/5435 [==============================] - 2s 416us/step - loss: 1.1560 - acc: 0.6167\n",
      "Epoch 51/200\n",
      "5435/5435 [==============================] - 2s 444us/step - loss: 1.1543 - acc: 0.6040\n",
      "Epoch 52/200\n",
      "5435/5435 [==============================] - 2s 458us/step - loss: 1.1371 - acc: 0.6156\n",
      "Epoch 53/200\n",
      "5435/5435 [==============================] - 3s 593us/step - loss: 1.1248 - acc: 0.6197\n",
      "Epoch 54/200\n",
      "5435/5435 [==============================] - 3s 561us/step - loss: 1.1172 - acc: 0.6195\n",
      "Epoch 55/200\n",
      "5435/5435 [==============================] - 3s 490us/step - loss: 1.1239 - acc: 0.6158\n",
      "Epoch 56/200\n",
      "5435/5435 [==============================] - 2s 389us/step - loss: 1.0974 - acc: 0.6272\n",
      "Epoch 57/200\n",
      "5435/5435 [==============================] - 2s 381us/step - loss: 1.0889 - acc: 0.6261\n",
      "Epoch 58/200\n",
      "5435/5435 [==============================] - 2s 376us/step - loss: 1.0634 - acc: 0.6385\n",
      "Epoch 59/200\n",
      "5435/5435 [==============================] - 2s 401us/step - loss: 1.0797 - acc: 0.6350\n",
      "Epoch 60/200\n",
      "5435/5435 [==============================] - 2s 457us/step - loss: 1.0631 - acc: 0.6519\n",
      "Epoch 61/200\n",
      "5435/5435 [==============================] - 2s 426us/step - loss: 1.0510 - acc: 0.6466\n",
      "Epoch 62/200\n",
      "5435/5435 [==============================] - 2s 434us/step - loss: 1.0319 - acc: 0.6557\n",
      "Epoch 63/200\n",
      "5435/5435 [==============================] - 2s 432us/step - loss: 1.0540 - acc: 0.6362\n",
      "Epoch 64/200\n",
      "5435/5435 [==============================] - 2s 439us/step - loss: 1.0356 - acc: 0.6545\n",
      "Epoch 65/200\n",
      "5435/5435 [==============================] - 2s 430us/step - loss: 1.0501 - acc: 0.6495\n",
      "Epoch 66/200\n",
      "5435/5435 [==============================] - 2s 454us/step - loss: 1.0098 - acc: 0.6541\n",
      "Epoch 67/200\n",
      "5435/5435 [==============================] - 2s 437us/step - loss: 1.0267 - acc: 0.6484\n",
      "Epoch 68/200\n",
      "5435/5435 [==============================] - 2s 447us/step - loss: 1.0019 - acc: 0.6543\n",
      "Epoch 69/200\n",
      "5435/5435 [==============================] - 2s 436us/step - loss: 1.0042 - acc: 0.6581\n",
      "Epoch 70/200\n",
      "5435/5435 [==============================] - 2s 455us/step - loss: 1.0079 - acc: 0.6567\n",
      "Epoch 71/200\n",
      "5435/5435 [==============================] - 2s 448us/step - loss: 0.9986 - acc: 0.6675\n",
      "Epoch 72/200\n",
      "5435/5435 [==============================] - 2s 421us/step - loss: 0.9733 - acc: 0.6822\n",
      "Epoch 73/200\n",
      "5435/5435 [==============================] - 2s 451us/step - loss: 0.9841 - acc: 0.6692\n",
      "Epoch 74/200\n",
      "5435/5435 [==============================] - 2s 426us/step - loss: 0.9669 - acc: 0.6764\n",
      "Epoch 75/200\n",
      "5435/5435 [==============================] - 3s 471us/step - loss: 0.9468 - acc: 0.6852\n",
      "Epoch 76/200\n",
      "5435/5435 [==============================] - 2s 424us/step - loss: 0.9604 - acc: 0.6804\n",
      "Epoch 77/200\n",
      "5435/5435 [==============================] - 2s 443us/step - loss: 0.9346 - acc: 0.6887\n",
      "Epoch 78/200\n",
      "5435/5435 [==============================] - 2s 435us/step - loss: 0.9423 - acc: 0.6885\n",
      "Epoch 79/200\n",
      "5435/5435 [==============================] - 2s 438us/step - loss: 0.9371 - acc: 0.6870\n",
      "Epoch 80/200\n",
      "5435/5435 [==============================] - 2s 423us/step - loss: 0.9288 - acc: 0.6931\n",
      "Epoch 81/200\n",
      "5435/5435 [==============================] - 2s 449us/step - loss: 0.9290 - acc: 0.6924\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - 2s 430us/step - loss: 0.9241 - acc: 0.6857\n",
      "Epoch 83/200\n",
      "5435/5435 [==============================] - 2s 451us/step - loss: 0.9362 - acc: 0.6894\n",
      "Epoch 84/200\n",
      "5435/5435 [==============================] - 2s 427us/step - loss: 0.9091 - acc: 0.6955\n",
      "Epoch 85/200\n",
      "5435/5435 [==============================] - 2s 437us/step - loss: 0.9038 - acc: 0.6971\n",
      "Epoch 86/200\n",
      "5435/5435 [==============================] - 2s 439us/step - loss: 0.8987 - acc: 0.7019\n",
      "Epoch 87/200\n",
      "5435/5435 [==============================] - 2s 426us/step - loss: 0.9030 - acc: 0.6949\n",
      "Epoch 88/200\n",
      "5435/5435 [==============================] - 2s 418us/step - loss: 0.8901 - acc: 0.7056\n",
      "Epoch 89/200\n",
      "5435/5435 [==============================] - 2s 442us/step - loss: 0.8749 - acc: 0.7100\n",
      "Epoch 90/200\n",
      "5435/5435 [==============================] - 2s 450us/step - loss: 0.8809 - acc: 0.7017\n",
      "Epoch 91/200\n",
      "5435/5435 [==============================] - 3s 484us/step - loss: 0.8696 - acc: 0.7091\n",
      "Epoch 92/200\n",
      "5435/5435 [==============================] - 2s 445us/step - loss: 0.8635 - acc: 0.7098\n",
      "Epoch 93/200\n",
      "5435/5435 [==============================] - 2s 441us/step - loss: 0.8577 - acc: 0.7139\n",
      "Epoch 94/200\n",
      "5435/5435 [==============================] - 2s 443us/step - loss: 0.8676 - acc: 0.7128\n",
      "Epoch 95/200\n",
      "5435/5435 [==============================] - 3s 469us/step - loss: 0.8547 - acc: 0.7185\n",
      "Epoch 96/200\n",
      "5435/5435 [==============================] - 2s 458us/step - loss: 0.8447 - acc: 0.7152\n",
      "Epoch 97/200\n",
      "5435/5435 [==============================] - 2s 445us/step - loss: 0.8491 - acc: 0.7222\n",
      "Epoch 98/200\n",
      "5435/5435 [==============================] - 2s 443us/step - loss: 0.8394 - acc: 0.7227\n",
      "Epoch 99/200\n",
      "5435/5435 [==============================] - 2s 448us/step - loss: 0.8392 - acc: 0.7253\n",
      "Epoch 100/200\n",
      "5435/5435 [==============================] - 2s 446us/step - loss: 0.8323 - acc: 0.7290\n",
      "Epoch 101/200\n",
      "5435/5435 [==============================] - 2s 436us/step - loss: 0.8140 - acc: 0.7314\n",
      "Epoch 102/200\n",
      "5435/5435 [==============================] - 2s 445us/step - loss: 0.8202 - acc: 0.7268\n",
      "Epoch 103/200\n",
      "5435/5435 [==============================] - 3s 493us/step - loss: 0.8201 - acc: 0.7244\n",
      "Epoch 104/200\n",
      "5435/5435 [==============================] - 3s 505us/step - loss: 0.8143 - acc: 0.7316\n",
      "Epoch 105/200\n",
      "5435/5435 [==============================] - 2s 435us/step - loss: 0.8102 - acc: 0.7330\n",
      "Epoch 106/200\n",
      "5435/5435 [==============================] - 2s 436us/step - loss: 0.8012 - acc: 0.7387\n",
      "Epoch 107/200\n",
      "5435/5435 [==============================] - 2s 427us/step - loss: 0.7953 - acc: 0.7443\n",
      "Epoch 108/200\n",
      "5435/5435 [==============================] - 2s 428us/step - loss: 0.7958 - acc: 0.7424\n",
      "Epoch 109/200\n",
      "5435/5435 [==============================] - 2s 430us/step - loss: 0.7791 - acc: 0.7426\n",
      "Epoch 110/200\n",
      "5435/5435 [==============================] - 2s 430us/step - loss: 0.7834 - acc: 0.7382\n",
      "Epoch 111/200\n",
      "5435/5435 [==============================] - 2s 438us/step - loss: 0.7938 - acc: 0.7288\n",
      "Epoch 112/200\n",
      "5435/5435 [==============================] - 2s 431us/step - loss: 0.7909 - acc: 0.7398\n",
      "Epoch 113/200\n",
      "5435/5435 [==============================] - 2s 436us/step - loss: 0.7811 - acc: 0.7465\n",
      "Epoch 114/200\n",
      "5435/5435 [==============================] - 2s 441us/step - loss: 0.7705 - acc: 0.7472\n",
      "Epoch 115/200\n",
      "5435/5435 [==============================] - 2s 422us/step - loss: 0.7746 - acc: 0.7408\n",
      "Epoch 116/200\n",
      "5435/5435 [==============================] - 2s 435us/step - loss: 0.7723 - acc: 0.7435\n",
      "Epoch 117/200\n",
      "5435/5435 [==============================] - 2s 434us/step - loss: 0.7687 - acc: 0.7472\n",
      "Epoch 118/200\n",
      "5435/5435 [==============================] - 2s 429us/step - loss: 0.7476 - acc: 0.7529\n",
      "Epoch 119/200\n",
      "5435/5435 [==============================] - 2s 428us/step - loss: 0.7649 - acc: 0.7457\n",
      "Epoch 120/200\n",
      "5435/5435 [==============================] - 2s 433us/step - loss: 0.7484 - acc: 0.7512\n",
      "Epoch 121/200\n",
      "5435/5435 [==============================] - 2s 427us/step - loss: 0.7463 - acc: 0.7557\n",
      "Epoch 122/200\n",
      "5435/5435 [==============================] - 2s 422us/step - loss: 0.7486 - acc: 0.7483\n",
      "Epoch 123/200\n",
      "5435/5435 [==============================] - 2s 424us/step - loss: 0.7507 - acc: 0.7461\n",
      "Epoch 124/200\n",
      "5435/5435 [==============================] - 2s 430us/step - loss: 0.7452 - acc: 0.7498\n",
      "Epoch 125/200\n",
      "5435/5435 [==============================] - 2s 432us/step - loss: 0.7388 - acc: 0.7522\n",
      "Epoch 126/200\n",
      "5435/5435 [==============================] - 2s 433us/step - loss: 0.7379 - acc: 0.7546\n",
      "Epoch 127/200\n",
      "5435/5435 [==============================] - 2s 431us/step - loss: 0.7384 - acc: 0.7533\n",
      "Epoch 128/200\n",
      "5435/5435 [==============================] - 2s 430us/step - loss: 0.7168 - acc: 0.7647\n",
      "Epoch 129/200\n",
      "5435/5435 [==============================] - 2s 431us/step - loss: 0.7362 - acc: 0.7579\n",
      "Epoch 130/200\n",
      "5435/5435 [==============================] - 2s 419us/step - loss: 0.7342 - acc: 0.7590\n",
      "Epoch 131/200\n",
      "5435/5435 [==============================] - 2s 429us/step - loss: 0.7223 - acc: 0.7614\n",
      "Epoch 132/200\n",
      "5435/5435 [==============================] - 2s 437us/step - loss: 0.7234 - acc: 0.7580\n",
      "Epoch 133/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 0.7022 - acc: 0.7698\n",
      "Epoch 134/200\n",
      "5435/5435 [==============================] - 2s 328us/step - loss: 0.7019 - acc: 0.7650\n",
      "Epoch 135/200\n",
      "5435/5435 [==============================] - 2s 341us/step - loss: 0.6946 - acc: 0.7711\n",
      "Epoch 136/200\n",
      "5435/5435 [==============================] - 2s 324us/step - loss: 0.7076 - acc: 0.7678\n",
      "Epoch 137/200\n",
      "5435/5435 [==============================] - 2s 332us/step - loss: 0.6866 - acc: 0.7722\n",
      "Epoch 138/200\n",
      "5435/5435 [==============================] - 2s 325us/step - loss: 0.6952 - acc: 0.7715\n",
      "Epoch 139/200\n",
      "5435/5435 [==============================] - 2s 369us/step - loss: 0.6811 - acc: 0.7722\n",
      "Epoch 140/200\n",
      "5435/5435 [==============================] - 3s 531us/step - loss: 0.6821 - acc: 0.7772\n",
      "Epoch 141/200\n",
      "5435/5435 [==============================] - 2s 428us/step - loss: 0.6887 - acc: 0.7718\n",
      "Epoch 142/200\n",
      "5435/5435 [==============================] - 2s 429us/step - loss: 0.6852 - acc: 0.7753\n",
      "Epoch 143/200\n",
      "5435/5435 [==============================] - 2s 425us/step - loss: 0.6769 - acc: 0.7763\n",
      "Epoch 144/200\n",
      "5435/5435 [==============================] - 2s 421us/step - loss: 0.6709 - acc: 0.7812\n",
      "Epoch 145/200\n",
      "5435/5435 [==============================] - 2s 435us/step - loss: 0.6929 - acc: 0.7739\n",
      "Epoch 146/200\n",
      "5435/5435 [==============================] - 2s 436us/step - loss: 0.6691 - acc: 0.7770\n",
      "Epoch 147/200\n",
      "5435/5435 [==============================] - 2s 412us/step - loss: 0.6722 - acc: 0.7746\n",
      "Epoch 148/200\n",
      "5435/5435 [==============================] - 2s 405us/step - loss: 0.6721 - acc: 0.7779\n",
      "Epoch 149/200\n",
      "5435/5435 [==============================] - 2s 409us/step - loss: 0.6716 - acc: 0.7809\n",
      "Epoch 150/200\n",
      "5435/5435 [==============================] - 2s 412us/step - loss: 0.6507 - acc: 0.7871\n",
      "Epoch 151/200\n",
      "5435/5435 [==============================] - 2s 414us/step - loss: 0.6571 - acc: 0.7796\n",
      "Epoch 152/200\n",
      "5435/5435 [==============================] - 2s 409us/step - loss: 0.6579 - acc: 0.7829\n",
      "Epoch 153/200\n",
      "5435/5435 [==============================] - 2s 411us/step - loss: 0.6568 - acc: 0.7794\n",
      "Epoch 154/200\n",
      "5435/5435 [==============================] - 3s 466us/step - loss: 0.6399 - acc: 0.7855\n",
      "Epoch 155/200\n",
      "5435/5435 [==============================] - 3s 499us/step - loss: 0.6313 - acc: 0.7925\n",
      "Epoch 156/200\n",
      "5435/5435 [==============================] - 3s 490us/step - loss: 0.6464 - acc: 0.7834\n",
      "Epoch 157/200\n",
      "5435/5435 [==============================] - 2s 388us/step - loss: 0.6356 - acc: 0.7917\n",
      "Epoch 158/200\n",
      "5435/5435 [==============================] - 3s 463us/step - loss: 0.6295 - acc: 0.7836\n",
      "Epoch 159/200\n",
      "5435/5435 [==============================] - 3s 473us/step - loss: 0.6231 - acc: 0.7967\n",
      "Epoch 160/200\n",
      "5435/5435 [==============================] - 3s 477us/step - loss: 0.6285 - acc: 0.7987\n",
      "Epoch 161/200\n",
      "5435/5435 [==============================] - 3s 481us/step - loss: 0.6403 - acc: 0.7921\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - 2s 343us/step - loss: 0.6326 - acc: 0.7856\n",
      "Epoch 163/200\n",
      "5435/5435 [==============================] - 2s 325us/step - loss: 0.6172 - acc: 0.7899\n",
      "Epoch 164/200\n",
      "5435/5435 [==============================] - 2s 325us/step - loss: 0.6214 - acc: 0.7943\n",
      "Epoch 165/200\n",
      "5435/5435 [==============================] - 2s 325us/step - loss: 0.6207 - acc: 0.7932\n",
      "Epoch 166/200\n",
      "5435/5435 [==============================] - 2s 326us/step - loss: 0.6099 - acc: 0.7937\n",
      "Epoch 167/200\n",
      "5435/5435 [==============================] - 2s 325us/step - loss: 0.6118 - acc: 0.7996\n",
      "Epoch 168/200\n",
      "5435/5435 [==============================] - 2s 324us/step - loss: 0.6142 - acc: 0.7947\n",
      "Epoch 169/200\n",
      "5435/5435 [==============================] - 2s 321us/step - loss: 0.6073 - acc: 0.7961\n",
      "Epoch 170/200\n",
      "5435/5435 [==============================] - 2s 323us/step - loss: 0.6143 - acc: 0.7893\n",
      "Epoch 171/200\n",
      "5435/5435 [==============================] - 2s 325us/step - loss: 0.6053 - acc: 0.7961\n",
      "Epoch 172/200\n",
      "5435/5435 [==============================] - 2s 340us/step - loss: 0.6024 - acc: 0.8081\n",
      "Epoch 173/200\n",
      "5435/5435 [==============================] - 2s 323us/step - loss: 0.5828 - acc: 0.8028\n",
      "Epoch 174/200\n",
      "5435/5435 [==============================] - 2s 323us/step - loss: 0.6047 - acc: 0.7923\n",
      "Epoch 175/200\n",
      "5435/5435 [==============================] - 2s 320us/step - loss: 0.5986 - acc: 0.8039\n",
      "Epoch 176/200\n",
      "5435/5435 [==============================] - 2s 323us/step - loss: 0.5942 - acc: 0.7987\n",
      "Epoch 177/200\n",
      "5435/5435 [==============================] - 2s 326us/step - loss: 0.5981 - acc: 0.8026\n",
      "Epoch 178/200\n",
      "5435/5435 [==============================] - 2s 327us/step - loss: 0.5801 - acc: 0.8064\n",
      "Epoch 179/200\n",
      "5435/5435 [==============================] - 2s 337us/step - loss: 0.5755 - acc: 0.8072\n",
      "Epoch 180/200\n",
      "5435/5435 [==============================] - 2s 326us/step - loss: 0.5815 - acc: 0.8046\n",
      "Epoch 181/200\n",
      "5435/5435 [==============================] - 2s 325us/step - loss: 0.5692 - acc: 0.8116\n",
      "Epoch 182/200\n",
      "5435/5435 [==============================] - 2s 338us/step - loss: 0.5800 - acc: 0.8079\n",
      "Epoch 183/200\n",
      "5435/5435 [==============================] - 2s 402us/step - loss: 0.5884 - acc: 0.8059\n",
      "Epoch 184/200\n",
      "5435/5435 [==============================] - 2s 364us/step - loss: 0.5718 - acc: 0.8171\n",
      "Epoch 185/200\n",
      "5435/5435 [==============================] - 2s 346us/step - loss: 0.5742 - acc: 0.8061\n",
      "Epoch 186/200\n",
      "5435/5435 [==============================] - 2s 358us/step - loss: 0.5529 - acc: 0.8072\n",
      "Epoch 187/200\n",
      "5435/5435 [==============================] - 2s 349us/step - loss: 0.5812 - acc: 0.8055\n",
      "Epoch 188/200\n",
      "5435/5435 [==============================] - 2s 350us/step - loss: 0.5547 - acc: 0.8160\n",
      "Epoch 189/200\n",
      "5435/5435 [==============================] - 3s 555us/step - loss: 0.5573 - acc: 0.8145\n",
      "Epoch 190/200\n",
      "5435/5435 [==============================] - 2s 414us/step - loss: 0.5617 - acc: 0.8147\n",
      "Epoch 191/200\n",
      "5435/5435 [==============================] - 2s 399us/step - loss: 0.5649 - acc: 0.8151\n",
      "Epoch 192/200\n",
      "5435/5435 [==============================] - 2s 418us/step - loss: 0.5533 - acc: 0.8169\n",
      "Epoch 193/200\n",
      "5435/5435 [==============================] - 2s 329us/step - loss: 0.5597 - acc: 0.8142\n",
      "Epoch 194/200\n",
      "5435/5435 [==============================] - 2s 327us/step - loss: 0.5362 - acc: 0.8241\n",
      "Epoch 195/200\n",
      "5435/5435 [==============================] - 2s 326us/step - loss: 0.5476 - acc: 0.8204\n",
      "Epoch 196/200\n",
      "5435/5435 [==============================] - 2s 326us/step - loss: 0.5529 - acc: 0.8149\n",
      "Epoch 197/200\n",
      "5435/5435 [==============================] - 2s 325us/step - loss: 0.5400 - acc: 0.8202\n",
      "Epoch 198/200\n",
      "5435/5435 [==============================] - 2s 330us/step - loss: 0.5515 - acc: 0.8123\n",
      "Epoch 199/200\n",
      "5435/5435 [==============================] - 2s 324us/step - loss: 0.5527 - acc: 0.8169\n",
      "Epoch 200/200\n",
      "5435/5435 [==============================] - 2s 325us/step - loss: 0.5463 - acc: 0.8138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', <keras.wrappers.scikit_learn.KerasClassifier object at 0x1c3b52b630>)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building a pipeline and fitting it\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=200, batch_size=1000, verbose=1, )))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "pipeline.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "3409/3409 [==============================] - 2s 655us/step - loss: 3.2748 - acc: 0.1182\n",
      "Epoch 2/800\n",
      "3409/3409 [==============================] - 2s 455us/step - loss: 2.5730 - acc: 0.1496\n",
      "Epoch 3/800\n",
      "3409/3409 [==============================] - 2s 469us/step - loss: 2.3901 - acc: 0.1678\n",
      "Epoch 4/800\n",
      "3409/3409 [==============================] - 2s 452us/step - loss: 2.3325 - acc: 0.1725\n",
      "Epoch 5/800\n",
      "3409/3409 [==============================] - 2s 444us/step - loss: 2.2721 - acc: 0.1874\n",
      "Epoch 6/800\n",
      "3409/3409 [==============================] - 2s 450us/step - loss: 2.2343 - acc: 0.1874\n",
      "Epoch 7/800\n",
      "3409/3409 [==============================] - 2s 444us/step - loss: 2.1980 - acc: 0.2027\n",
      "Epoch 8/800\n",
      "3409/3409 [==============================] - 2s 451us/step - loss: 2.2054 - acc: 0.1968\n",
      "Epoch 9/800\n",
      "3409/3409 [==============================] - 2s 475us/step - loss: 2.1625 - acc: 0.2206\n",
      "Epoch 10/800\n",
      "3409/3409 [==============================] - 2s 488us/step - loss: 2.1344 - acc: 0.2309\n",
      "Epoch 11/800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b5f90b93e70f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cross-validating pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#cross-validating pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## creating a test dataframe\n",
    "\n",
    "test = pd.read_csv('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/test.csv')\n",
    "\n",
    "def feature_extractor_t(row):\n",
    "    # function to load files and extract features\n",
    "    file_name = os.path.join(os.path.abspath('/Users/pranjal/Desktop/audio_feature_extraction/case_study_data/'),\n",
    "                            'Test', str(row.ID) + '.wav')\n",
    "\n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        \n",
    "        \n",
    "        #calculating MFCC feature\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=39).T,axis=0) \n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "\n",
    "    features = mfccs\n",
    "    \n",
    "\n",
    "    return features.tolist()\n",
    "\n",
    "new_test = test.apply(feature_extractor_t, axis=1)\n",
    "new_test.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3297/3297 [==============================] - 1s 157us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_t = np.array(new_test.tolist())\n",
    "\n",
    "lb.inverse_transform\n",
    "\n",
    "test['Class']=lb.inverse_transform(pipeline.predict(X_t))\n",
    "\n",
    "test.to_csv('predictions_5.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
